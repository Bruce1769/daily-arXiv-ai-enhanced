<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 33]
- [cs.CL](#cs.CL) [Total: 23]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.IT](#cs.IT) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文针对视频描述模型在细粒度运动细节描述和幻觉问题上的局限性，提出了KPM-Bench数据集和MoPE算法，通过运动解析和提取技术改善运动中心视频描述的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述模型在准确描述细粒度运动细节方面存在显著限制，特别是在运动中心视频中，对复杂动作和肢体动态的精确描述往往被忽视，同时存在严重的幻觉问题。

Method: 1) 开发自动化标注流程，整合基于运动学的运动计算和语言解析；2) 构建KPM-Bench数据集，包含细粒度视频-描述对、运动理解问答对和幻觉评估集；3) 提出基于语言的运动解析和提取(MoPE)算法；4) 将MoPE集成到GRPO后训练框架中。

Result: 1) 创建了KPM-Bench开源数据集；2) 开发了MoPE算法能够从文本描述中准确提取运动特定属性；3) 提出了独立的幻觉评估指标；4) 通过MoPE集成显著改善了运动中心视频描述模型的可靠性。

Conclusion: 该研究通过创新的数据集构建和算法设计，有效解决了视频描述中的细粒度运动理解和幻觉问题，为运动中心视频理解提供了系统性的解决方案，显著提升了模型的准确性和可靠性。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了3D-HIW数据集和CLUTCH系统，用于解决野外环境下3D手部动作建模的挑战，在文本到手部动作生成和动作描述任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手部动作在日常生活中至关重要，但现有方法依赖工作室采集的数据集，动作和上下文有限，难以扩展到野外环境。同时，现有模型在动画保真度和文本-动作对齐方面存在不足。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和3D手部跟踪器，从大量第一人称动作视频中提取32K个3D手部动作序列和对应文本。2) 提出CLUTCH系统：包含SHIFT（基于部分模态分解的VQ-VAE架构）用于手部动作标记化，以及几何精炼阶段用于微调LLM。

Result: 在文本到手部动作生成和动作到文本描述任务上实现了最先进的性能，为可扩展的野外手部动作建模建立了首个基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外环境下手部动作建模的挑战，为自然手部动作生成和描述提供了有效的解决方案。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，直接从H&E切片预测HER2表达水平，避免重建伪影并提高效率


<details>
  <summary>Details</summary>
Motivation: 标准IHC染色资源密集、昂贵耗时且在许多地区不可用，现有基于H&E切片的虚拟染色方法计算昂贵且易产生重建伪影传播诊断错误

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，学习将形态学H&E特征直接映射到分子潜在空间，通过教师IHC编码器指导训练，使用核分布和膜染色强度等任务特定领域知识进行正则化

Result: 在公开BCI数据集上的广泛实验表明，LGD-Net达到最先进性能，显著优于基线方法，同时支持使用单模态H&E输入进行高效推理

Conclusion: LGD-Net通过跨模态特征幻觉而非像素级生成，有效解决了虚拟染色方法的计算成本和重建伪影问题，为HER2表达评估提供了高效准确的替代方案

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [4] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了一种无需额外训练、基于现有基础模型的遥感图像文本引导分割方法，结合对比式和生成式视觉语言模型与SAM，实现完全零样本或轻量级LoRA调优的分割流程。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型和视觉基础模型为遥感图像零样本文本引导分割提供了新机会，但现有方法大多依赖额外的可训练组件，限制了泛化能力和实际应用性。本研究旨在探索仅依靠现有基础模型、无需额外训练就能实现文本引导遥感分割的可能性。

Method: 提出两种方法：1）对比式方法使用CLIP作为SAM网格提议的掩码选择器，实现完全零样本的开放词汇语义分割；2）生成式方法使用GPT-4V（零样本）和LoRA调优的Qwen-VL模型生成点击提示给SAM，实现推理和指代分割。两种方法都无需额外训练或仅需轻量级LoRA调优。

Result: 在19个遥感基准测试（包括开放词汇、指代和基于推理的任务）上进行广泛实验，对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法中LoRA调优的Qwen-VL模型表现最佳。

Conclusion: 研究表明，仅依靠现有基础模型、无需额外训练就能实现有效的文本引导遥感分割，提出的方法在多种任务上展现出强大能力，为遥感分割的实际应用提供了更通用的解决方案。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [5] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了首个视频查询性能预测基准VQPP，包含两个文本到视频检索数据集和两个CBVR系统，共56K文本查询和51K视频，为视频领域的QPP研究提供了标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测在信息检索中具有重要应用，但在基于内容的视频检索领域研究不足。目前QPP主要在文本和图像检索中研究，视频检索领域的QPP基准缺失，阻碍了该领域的发展。

Method: 提出了VQPP基准，包含两个文本到视频检索数据集和两个CBVR系统。探索了多种检索前和检索后性能预测器，并使用最佳检索前预测器作为奖励模型，通过直接偏好优化训练大型语言模型进行查询重写。

Result: 检索前预测器获得了有竞争力的性能，能够在检索步骤之前实现应用。VQPP基准包含56K文本查询和51K视频，提供了官方训练、验证和测试划分，促进了直接比较和可复现结果。

Conclusion: VQPP是首个视频查询性能预测基准，为视频领域的QPP研究提供了标准化框架。研究表明检索前预测器具有实用价值，并展示了VQPP在查询重写等应用中的潜力。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [6] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文对Liu和Szirányi的手势识别方法进行了方法学分析，指出其评估协议存在严重的数据泄露问题，导致报告的高准确率指标不反映对未见个体的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是揭示Liu和Szirányi手势识别方法评估协议中的缺陷，特别是数据泄露问题，强调在基于视觉的手势识别研究中，尤其是对于无人机-人交互等需要可靠识别未见个体手势的应用，主体独立数据划分的重要性。

Method: 通过分析已发表的混淆矩阵、学习曲线和数据集构建方法，作者展示了Liu和Szirányi使用的帧级随机训练-测试划分会不可避免地混合来自同一主体的样本，导致严重的数据泄露。作者采用方法学分析方法，检验评估协议的有效性。

Result: 分析结果表明，报告的高准确率指标源于数据泄露问题，评估并未测量对未见个体的泛化能力。通过检查混淆矩阵和学习曲线，作者证明了评估协议存在严重缺陷。

Conclusion: 该研究强调了在基于视觉的手势识别研究中主体独立数据划分的重要性，特别是对于需要可靠识别未见个体手势的应用场景。研究结果提醒研究者需要采用更严格的评估协议来确保方法的实际泛化能力。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [7] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出了一种用于长视频理解的新型端到端框架，包含基于信息密度的自适应视频采样器和基于自动编码器的时空视频压缩器，结合多模态大语言模型，有效处理长视频的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 随着视频骨干架构的进步和大语言模型的成功，分析长达数十分钟的长视频变得可行且普遍。然而，视频序列固有的冗余性给现有模型带来两大挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出端到端的长视频理解框架，包含：1) 基于信息密度的自适应视频采样器(AVS)，自适应捕捉不同时长视频的关键信息；2) 基于自动编码器的时空视频压缩器(SVC)，实现高压缩率同时保留关键判别信息；3) 与多模态大语言模型(MLLM)集成。

Result: 该框架在多个基准测试中表现出色，不仅在长视频理解任务上表现优异，在标准视频理解基准测试中也取得良好结果，证明了方法在处理长视频复杂性方面的有效性和通用性。

Conclusion: 提出的框架通过自适应采样和高效压缩机制，有效解决了长视频理解中的冗余问题，为处理长时间视频序列提供了一种高效且通用的解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [8] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 该研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器能显著提升细粒度分类性能，而预训练阶段对模型权重解冻时尤为重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种视觉问答基准测试中取得了显著进展，但在传统的图像分类基准测试（特别是细粒度视觉知识测试）上表现不佳。研究者希望探究这种性能差异的原因，并找出影响细粒度视觉理解的关键因素。

Method: 研究测试了大量最新的视觉语言模型在细粒度分类基准上的表现，通过一系列消融实验分析性能差异的原因。实验重点关注不同组件（如LLM和视觉编码器）对性能的影响，以及预训练阶段对模型权重解冻的重要性。

Result: 研究发现：1）使用更好的大型语言模型能同等提升所有基准测试分数；2）更好的视觉编码器能不成比例地显著提升细粒度分类性能；3）预训练阶段对细粒度性能至关重要，特别是在语言模型权重解冻的情况下。

Conclusion: 该研究揭示了视觉语言模型中细粒度视觉理解能力的关键影响因素，为提升视觉语言模型的细粒度视觉理解和视觉中心能力提供了重要见解和方向。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [9] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该研究提出了一种多模态深度重建框架，利用极稀疏的雷达或LiDAR测距数据生成密集深度图，作为扩散模型新视角合成的几何条件，显著提升单图像新视角合成的几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的单图像新视角合成方法依赖于单目深度估计的几何信息，但在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计的可靠性有限，限制了合成视图的质量和一致性。

Method: 提出多模态深度重建框架，利用极稀疏的测距数据（如汽车雷达或LiDAR），在角度域中使用局部高斯过程建模深度，实现计算高效推理并显式量化观测有限区域的不确定性。重建的深度和不确定性可直接替代现有扩散渲染流程中的单目深度估计器。

Result: 在真实世界多模态驾驶场景实验中，用稀疏测距重建深度替代纯视觉深度，显著提升了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 研究强调了可靠几何先验对基于扩散的视图合成的重要性，并展示了即使在极端稀疏情况下，多模态传感也能带来实际效益。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [10] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑型视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer依赖位置嵌入和类别标记编码固定的空间先验，这在自然图像中有效，但在医学图像中可能阻碍泛化能力，因为医学图像的空间布局信息较弱或不一致。

Method: 提出ZACH-ViT，移除位置嵌入和[CLS]标记，通过全局平均池化处理补丁表示实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格控制参数预算。

Result: 在7个MedMNIST数据集上进行评估，ZACH-ViT（0.25M参数）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL竞争，但在具有强解剖先验的数据集（OCTMNIST、OrganAMNIST）上优势减弱。尽管尺寸小且无预训练，仍保持亚秒级推理时间。

Conclusion: 将架构归纳偏置与数据结构对齐比追求通用基准优势更重要。ZACH-ViT在资源受限的临床环境中具有部署潜力，代码和模型已开源。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [11] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出基于记忆驱动的质量感知框架MQAF，通过建立存储失真模式的记忆库，动态切换双模式质量评估策略，减少对高质量参考图像的依赖，同时支持有参考和无参考图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估方法依赖参考图像质量，限制了在理想参考源不可用的实际应用。受人类视觉系统能够积累视觉记忆进行质量评估的启发，需要开发减少对高质量参考图像依赖的评估框架。

Method: 提出记忆驱动的质量感知框架MQAF：建立存储失真模式的记忆库，动态切换双模式评估策略。有参考图像时，通过自适应加权参考信息并与记忆库中的失真模式比较获得质量分数；无参考图像时，依赖记忆库中的失真模式推断图像质量。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时能够适应无参考和有参考两种任务。

Conclusion: MQAF框架通过模拟人类视觉记忆机制，有效减少了对高质量参考图像的依赖，在保持高精度评估的同时增强了实际应用能力，为图像质量评估提供了新的生物启发式解决方案。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [12] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: MUOT_3M是首个伪多模态水下目标跟踪基准，包含300万帧视频数据；MUTrack是基于SAM的多模态到单模态跟踪器，通过知识蒸馏实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人、生态监测和海洋探索至关重要，但现有基准数据集规模小且仅包含RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 1) 构建MUOT_3M基准：包含3030个视频的300万帧，标注32个跟踪属性、677个细粒度类别，提供RGB、增强RGB、估计深度和语言四种同步模态；2) 提出MUTrack跟踪器：基于SAM架构，采用视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识转移到单模态学生模型中。

Result: 在五个UOT基准上的广泛评估显示，MUTrack相比最强的SOTA基线，AUC提升高达8.40%，精度提升高达7.80%，同时运行速度达到24 FPS。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新的基础，解决了现有基准的局限性并提升了跟踪性能。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [13] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型的情感视觉定制任务（L-AVC），旨在通过编辑图像的主观情感内容来生成图像，并提出了高效精确的情感操纵方法（EPEM）来解决情感语义转换和情感无关内容保留的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要依赖各种控制信号（如语言、布局、边缘检测）与编辑图像之间的客观对齐，忽略了主观情感内容，且缺乏面向情感视觉定制的通用基础模型。因此需要开发能够有效编辑图像主观情感的方法。

Method: 提出高效精确情感操纵方法（EPEM），包含两个核心模块：1）高效情感间转换（EIC）模块，使LLM在编辑前后高效对齐情感语义转换；2）精确情感外保留（PER）模块，精确保留情感无关内容。

Result: 在构建的L-AVC数据集上进行综合实验评估，结果表明EPEM方法在L-AVC任务上优于多个最先进的基线方法，证明了情感信息对L-AVC的重要性以及EPEM在高效精确操纵情感信息方面的有效性。

Conclusion: 本文提出的L-AVC任务和EPEM方法填补了情感视觉定制领域的空白，通过多模态LLM实现了对图像主观情感的高效精确编辑，为情感感知的图像生成和定制提供了新的解决方案。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [14] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度安全导向视频理解任务（DeepSVU），旨在不仅识别和定位威胁，还要归因和评估威胁原因，并提出了统一物理世界正则化MoE方法来解决该任务中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注检测和定位威胁（如枪击、抢劫），但缺乏有效生成和评估威胁原因的能力。为填补这一空白，本文提出了深度安全导向视频理解任务。

Method: 提出统一物理世界正则化MoE方法，包含两个关键组件：统一物理世界增强MoE块和物理世界权衡正则化器，分别解决粗到细物理世界信息建模和自适应权衡这些因素的挑战。

Result: 在DeepSVU指令数据集（UCF-C指令和CUVA指令）上的大量实验表明，UPRM方法优于多个先进的视频大语言模型和非VLM方法，验证了粗到细物理世界信息在DeepSVU任务中的重要性。

Conclusion: 本文提出的DeepSVU任务和UPRM方法有效解决了安全导向视频理解中的深度分析需求，通过建模物理世界信息显著提升了威胁识别、定位和原因分析的能力。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [15] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR模块，一种无需训练、即插即用的方法，通过不确定性感知的观察信息重注入来提升VLA模型性能，无需额外数据收集或模块


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型增强方法通常需要额外的观察线索（如深度图、点云）或辅助模块（如物体检测器），这些方法需要昂贵的数据收集和额外训练。作者希望开发一种无需训练、即插即用的模块来提升VLA模型性能

Method: 提出不确定性感知观察重注入（UAOR）模块：当语言模型层表现出高不确定性（通过动作熵测量）时，通过注意力检索将关键观察信息重新注入到下一层的FFN中，帮助VLA模型在推理过程中更好地关注观察信息

Result: 综合实验表明，该方法在各种VLA模型上都能一致提升性能，涵盖仿真和真实世界任务，且开销极小。特别地，UAOR消除了对额外观察线索或模块的需求

Conclusion: UAOR是一种有效、无需训练、即插即用的模块，能够提升VLA模型的性能，使其成为现有VLA管道的通用实用插件

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [16] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出DCAG框架，通过同时操纵DiT注意力机制中的Key和Value通道实现无训练编辑强度控制，相比仅操作Key的方法在编辑保真度权衡上更精确。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的扩散图像编辑模型需要无训练编辑强度控制，但现有注意力操纵方法只关注Key空间来调节注意力路由，完全忽略了控制特征聚合的Value空间。

Method: 首先发现DiT多模态注意力层中Key和Value投影都表现出明显的偏置-增量结构，基于此提出DCAG框架，同时操纵Key通道（控制注意力位置）和Value通道（控制聚合内容），形成二维参数空间(δ_k, δ_v)。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上都优于仅使用Key指导的方法，在对象删除任务中LPIPS降低4.9%，在对象添加任务中LPIPS降低3.2%。

Conclusion: DCAG通过同时操纵Key和Value通道，实现了比任何单通道方法更精确的编辑-保真度权衡，Key通道通过非线性softmax函数提供粗粒度控制，Value通道通过线性加权求和提供细粒度补充。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [17] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出了一种基于分解-整合框架的少样本动作识别方法，利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型。


<details>
  <summary>Details</summary>
Motivation: 传统的少样本动作识别方法通常使用语义粗糙的类别名称作为辅助上下文来指导视觉特征学习，但动作名称提供的上下文过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识。

Method: 提出DiST框架，包含分解阶段和整合阶段。分解阶段将原始动作名称解耦为多样化的时空属性描述；整合阶段提出空间/时间知识补偿器（SKC/TKC）来发现判别性的对象级和帧级原型。SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 实验结果表明DiST在五个标准少样本动作识别数据集上取得了最先进的结果。

Conclusion: DiST通过利用大语言模型提供的解耦空间和时间知识，能够学习表达性多粒度原型，有效捕捉细粒度空间细节和多样化时间模式，从而提升少样本动作识别性能。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [18] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard：一种用于分散式监控中隐私保护身份检索的拓扑感知Transformer框架，通过自适应度量学习、空间条件注意力和差分隐私嵌入实现跨摄像头的人员重识别。


<details>
  <summary>Details</summary>
Motivation: 城市规模的人员重识别面临视角变化、遮挡和域偏移等挑战，同时需要遵守数据保护法规，防止原始图像共享。现有方法在隐私保护和跨视图对齐方面存在不足。

Method: 1. 分散自适应度量学习：根据特征分布调整实例级边界，增强类内紧凑性；2. 空间条件注意力：将GPS或部署平面图等粗略几何信息注入图自注意力，实现投影一致的跨视图对齐；3. 差分隐私嵌入映射与紧凑近似索引：支持安全且成本高效的部署。

Result: 在Market-1501等公开基准测试中，检索精度和查询吞吐量均优于强基线方法，数据库规模检索研究证实了框架的实际可行性。

Conclusion: CityGuard框架能够生成对视角变化、遮挡和域偏移鲁棒的描述符，在严格的差分隐私核算下实现隐私与效用的可调平衡，适用于隐私关键的城市身份匹配应用。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [19] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 该论文提出了一种两阶段训练策略，通过自监督预训练减少对昂贵BEV标注的依赖，在nuScenes数据集上仅用50%标注数据就能超越全监督基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV语义地图方法依赖昂贵且标注不一致的BEV地面真值，这限制了自动驾驶感知系统的可扩展性。需要一种减少标注依赖的方法来提升效率。

Method: 采用两阶段训练策略：1) 自监督预训练阶段，将BEVFormer预测结果可微分地重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，并加入时序一致性损失；2) 监督微调阶段，仅使用50%数据集进行微调。

Result: 在nuScenes数据集上，该方法比全监督基线模型提升高达2.5pp mIoU，同时将标注数据使用量减半，总训练时间减少三分之二，BEV分割质量显著提升。

Conclusion: 可微分重投影加相机视角伪标签能够产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了一条可扩展的路径，显著降低了数据标注成本和训练时间。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [20] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模、多作者的手写印地语数据集，包含531位作者抄写的6首传统印地语对句，旨在解决德瓦纳格里文字手写文本数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 德瓦纳格里文字手写文本在公开基准数据集中严重不足，现有资源规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和作者多样性，无法捕捉德瓦纳格里手写体连续、融合和结构复杂的特性。

Method: 创建DohaScript数据集，收集531位独特贡献者的手写印地语文本，设计为平行风格语料库，所有作者抄写相同的6首传统印地语对句，包含非识别性人口统计元数据、基于清晰度和分辨率的严格质量筛选以及页面级布局难度标注。

Result: 基线实验显示清晰的质量分离和对未见作者的强泛化能力，证明了数据集的可靠性和实用价值。数据集支持手写识别、作者识别、风格分析和生成建模等任务。

Conclusion: DohaScript旨在作为标准化、可复现的基准，推动低资源脚本环境下连续手写德瓦纳格里文本的研究进展。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [21] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：一个自动化构建的基准测试，用于评估视觉语言模型处理分布外数据的能力，包含4万实例级OOD样本，发现现有模型在常见类别上仍表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型通常在独立同分布假设下训练，但现实应用中常遇到分布外数据，这可能带来安全风险。目前缺乏全面评估VLM处理OOD数据能力的有效基准。

Method: 提出OODBench方法，采用最小人工验证的自动化方式构建基准测试，包含4万实例级OOD实例-类别对。同时提出可靠的自评估指标，使用从基础到高级的渐进式提示问题来更全面评估OOD数据对不同难度问题的影响。

Result: 当前视觉语言模型在OODBench上仍表现出显著性能下降，即使底层图像类别是常见的。研究还总结了重要发现和见解，以促进未来OOD数据获取和评估的研究。

Conclusion: OODBench为评估视觉语言模型处理分布外数据能力提供了有效基准，揭示了现有模型的局限性，并提出了改进的评估方法，为未来研究提供了重要基础。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [22] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT：一种无需训练的特征预测加速框架，通过线性多步方法预测扩散模型输出，实现最高5.54倍延迟降低，同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）在图像和视频生成中表现出色，但其迭代去噪过程计算成本高。现有免训练加速方法依赖特征缓存和重用，但可能导致潜在漂移和视觉质量下降。作者观察到模型输出在扩散轨迹的大部分阶段平滑演化，这为预测而非简单重用提供了理论基础。

Method: 提出PrediT框架：1）将特征预测建模为线性多步问题，使用经典线性多步方法从历史信息预测未来模型输出；2）在高动态区域激活校正器防止误差累积；3）动态步长调制机制通过监控特征变化率自适应调整预测范围。

Result: 在多种基于DiT的图像和视频生成模型上，PrediT实现了高达5.54倍的延迟降低，同时质量下降可忽略不计。广泛实验验证了该方法的有效性。

Conclusion: PrediT是一种有效的免训练加速框架，通过特征预测而非简单重用，在保持生成质量的同时显著降低扩散变换器的计算成本，为高效图像和视频生成提供了实用解决方案。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [23] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个用于短视频广告内容审核的框架，通过思维链推理与规则策略结合，利用强化学习优化模型，能有效检测多模态欺骗性内容。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性的视觉、语音和字幕内容，需要比社区安全过滤器更细粒度、基于策略的审核机制。

Method: 1. 融合思维链推理与基于规则的策略原则和批评引导奖励；2. 规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；3. 强化学习使用复合奖励平衡因果一致性与策略遵从性；4. 多任务架构建模模态内操纵和跨模态不匹配。

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面超越了强基线模型。

Conclusion: BLM-Guard框架为商业广告内容审核提供了有效的解决方案，通过结合思维链推理、规则策略和强化学习，显著提升了多模态欺骗性内容的检测能力。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [24] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法修正文本生成动作中的物理不合理性（如脚部漂浮），同时保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法在语义对齐方面进展迅速，但难以同时保证语义和物理真实性。现有方法生成的动画常出现物理不合理现象（如脚部漂浮），需要一种能修正物理不合理性同时保持语义一致性的解决方案。

Method: 提出Distortion-aware Motion Calibrator (DMC)后处理模块，采用自监督数据驱动方法。DMC学习在给定故意扭曲的动作和原始文本描述时，生成物理合理的动作，而不依赖复杂的物理建模。

Result: DMC在多个文本到动作生成模型上显著提升性能：在T2M上FID分数降低42.74%，在T2M-GPT上降低13.20%，同时获得最高的R-Precision。应用于高质量模型如MoMask时，穿透率减少33.0%，漂浮伪影更接近真实参考。

Conclusion: DMC作为一种有前景的后处理动作精炼框架，能够为各种文本到动作模型提供物理合理性修正，同时保持文本语义一致性，解决了现有方法在物理真实性方面的不足。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [25] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 本文首次研究离散图像分词器的对抗攻击脆弱性，提出高效、应用无关的攻击方法，并通过无监督对抗训练增强分词器鲁棒性，为安全多模态基础模型发展提供重要步骤。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中应用日益广泛，但相比CLIP编码器，其对抗攻击脆弱性尚未被探索。本文旨在填补这一研究空白，揭示分词器鲁棒性对下游任务的重要性。

Method: 1. 提出针对离散分词器的对抗攻击方法，旨在扰动特征提取并改变分词结果；2. 受鲁棒CLIP编码器研究启发，采用无监督对抗训练微调流行分词器，保持其他组件不变。

Result: 攻击方法计算高效、应用无关，在分类、多模态检索和字幕生成任务中均有效。防御方法显著提升对无监督和端到端监督攻击的鲁棒性，并能良好泛化到未见任务和数据。

Conclusion: 本研究首次揭示了离散图像分词器的对抗脆弱性，提出的无监督对抗训练方法相比监督方法更具通用性，为开发安全的多模态基础模型迈出了重要一步。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [26] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的新框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解问题，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多实例生成方法在空间布局和属性绑定方面已有进步，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是在防止实例间属性泄漏和精确匹配局部化文本描述方面存在局限。

Method: 提出DEIG框架，包含两个核心组件：1) 实例细节提取器(IDE)，将文本编码器嵌入转换为紧凑的实例感知表示；2) 细节融合模块(DFM)，应用基于实例的掩码注意力机制防止实例间属性泄漏。同时构建了包含详细组合实例描述的高质量数据集，并开发了DEIG-Bench基准测试。

Result: 实验表明DEIG在多个基准测试中，在空间一致性、语义准确性和组合泛化方面均优于现有方法。DEIG可作为即插即用模块，易于集成到标准基于扩散的流程中。

Conclusion: DEIG通过细粒度的实例感知表示和属性泄漏预防机制，能够生成与丰富局部化文本描述精确匹配的视觉连贯多实例场景，为多实例生成提供了有效的解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [27] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS框架通过多级条件编码和扩散对引导，结合全局草图结构和局部文本-草图对，提升时尚图像生成质量，并发布了首个多文本-草图对的时尚数据集Sketchy。


<details>
  <summary>Details</summary>
Motivation: 草图为时尚设计提供结构、轮廓和空间关系，文本描述补充材质、颜色和风格细节。有效结合文本和视觉模态需要在遵循草图视觉结构的同时利用文本的局部属性指导。

Method: 提出LOTS框架：1) 多级条件编码阶段：在共享潜在空间中独立编码局部特征，同时保持全局结构协调；2) 扩散对引导阶段：通过注意力机制在扩散模型的多步去噪过程中整合局部和全局条件。

Result: 创建了首个时尚数据集Sketchy，包含每张图像对应的多个文本-草图对。实验表明该方法在增强全局结构遵循的同时利用更丰富的局部语义指导，优于现有最先进方法。

Conclusion: LOTS框架通过结合全局草图引导和多个局部草图-文本对，显著提升了时尚图像生成的质量和可控性，为多模态时尚设计提供了有效解决方案。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [28] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS：一种用于手术场景重建的两阶段框架，通过扩散模型修复被器械遮挡的组织，并使用可学习变形模型的2D高斯泼溅技术实现高质量的3D重建。


<details>
  <summary>Details</summary>
Motivation: 实时重建可变形手术场景对推进机器人手术、改善外科医生引导和实现自动化至关重要。现有方法在遮挡区域的重建质量有限，且缺乏深度准确性评估，因为现有基准数据集缺少3D真实数据。

Method: 提出Diff2DGS两阶段框架：第一阶段使用基于扩散的视频模块，利用时间先验修复被手术器械遮挡的组织，保持高时空一致性；第二阶段采用带有可学习变形模型(LDM)的2D高斯泼溅技术，捕捉动态组织变形和解剖几何结构。

Result: 在EndoNeRF上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB PSNR，优于现有方法。实验表明仅优化图像质量不一定能获得最佳3D重建准确性，因此进一步优化深度质量以确保更准确的几何结构。

Conclusion: Diff2DGS在手术场景重建中实现了外观和几何结构的双重优化，通过两阶段方法有效处理遮挡问题，并在SCARED数据集上进行了定量深度准确性分析，为手术场景重建提供了更可靠的解决方案。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [29] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++是一个基于3D高斯泼溅的框架，用于解决多视角捕获中光照变化导致的色彩不一致问题，通过全局自适应亮度调整和局部像素级残差细化实现鲁棒的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现实环境中复杂的光照变化和相机成像管道的局限性，特别是在多视角捕获中，会导致光度不一致性，这违反了现代3D新视角合成方法（如NeRF和3DGS）所依赖的光度一致性假设，从而降低重建和渲染质量。

Method: 提出Luminance-GS++框架，结合全局视图自适应亮度调整和局部像素级残差细化进行精确色彩校正。设计无监督目标，联合强制执行亮度校正以及多视角几何和光度一致性，同时保持显式的3DGS表示形式。

Result: 在包括低光照、过曝光以及复杂亮度和色彩变化在内的挑战性场景中，该方法展现出最先进的性能，提高了重建保真度，同时保持了实时渲染效率。

Conclusion: Luminance-GS++通过创新的色彩校正方法有效解决了多视角捕获中的光度不一致问题，在保持3DGS实时渲染优势的同时，显著提升了在复杂光照条件下的新视角合成质量。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [30] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯-拉普拉斯算子(G-LoG)的双参数过滤方法，用于医学图像拓扑数据分析，相比单参数过滤显著提升性能，其拓扑特征训练的简单MLP模型性能可与复杂深度学习模型媲美。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的过滤方法来检测拓扑和几何特征是一个重要任务。本文旨在利用拉普拉斯高斯算子增强医学图像边界的能力，定义更适合多参数持久性模块的特征。

Method: 提出G-LoG（高斯-拉普拉斯高斯）双参数过滤方法：1）将体积图像建模为有界函数；2）利用拉普拉斯高斯算子增强图像边界；3）构建双参数过滤生成拓扑特征；4）证明持久性模块的交互距离相对于有界函数的最大范数是稳定的。

Result: 1）在MedMNIST数据集上的实验表明，双参数过滤显著优于单参数过滤；2）基于双参数过滤生成的拓扑特征训练的简单多层感知器（MLP），其性能可与Google AutoML Vision、ResNet、AutoKeras和auto-sklearn等复杂深度学习模型相媲美。

Conclusion: G-LoG双参数过滤方法为医学图像拓扑数据分析提供了有效的特征提取方案，其生成的拓扑特征能够支持简单模型达到与复杂深度学习模型相当的分类性能，证明了拓扑特征在医学图像分析中的实用价值。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [31] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出了一个基于头部和手部姿态控制的人为中心视频世界模型，用于扩展现实(XR)中的交互式虚拟环境生成


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型只能接受文本或键盘等粗粒度控制信号，无法响应用户真实世界运动跟踪，限制了在具身交互中的实用性

Method: 1) 评估现有扩散变换器条件策略；2) 提出有效的3D头部和手部姿态控制机制；3) 训练双向视频扩散模型教师；4) 蒸馏为因果交互系统生成第一人称虚拟环境

Result: 通过人类受试者评估显示，相比相关基线，该系统提高了任务性能，并显著提升了用户对执行动作的控制感知水平

Conclusion: 该人中心视频世界模型能够响应跟踪的头部和手部姿态，支持灵巧的手-物交互，为扩展现实中的具身交互提供了有效的生成模型解决方案

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [32] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力限制下进行室内导航的新基准，包含5种代表性智能体、45个真实室内场景、473个导航任务和2365个问答对，发现当前VLM在能力约束下导航性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界导航本质上受到智能体移动能力的限制（如扫地机器人无法爬楼梯，四足机器人可以），但现有视觉语言导航研究很少考虑智能体的具体物理和操作能力。需要评估VLM在考虑智能体能力约束下的导航性能。

Method: 提出了能力条件导航基准CapNav，定义了5种代表性人类和机器人智能体，每个智能体都有详细的物理尺寸、移动能力和环境交互能力描述。基准包含45个真实室内场景、473个导航任务和2365个问答对，用于测试VLM在考虑智能体能力限制下的导航决策能力。

Result: 评估了13个现代VLM，发现：1）当前VLM的导航性能随着移动约束收紧而急剧下降；2）即使是SOTA模型在处理需要空间维度推理的障碍类型时也表现不佳；3）VLM在能力感知导航方面存在显著挑战。

Conclusion: CapNav基准揭示了当前VLM在能力条件导航方面的局限性，为未来VLM在具身空间推理方面的发展提供了重要机会和方向，强调了开发能够理解智能体物理约束的导航系统的重要性。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [33] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过增加token预算、自适应选择策略和训练免费检索专家混合，显著提升了流式视频理解性能


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token数量，导致细粒度视觉细节丢失，且在处理密集视频流时存在查询-帧相似度随时间增加的问题，偏向检索后期帧

Method: 1. 增加token预算以实现更细粒度的时空理解；2. 引入自适应选择策略减少token冗余同时保留局部时空信息；3. 提出训练免费的检索专家混合，利用外部模型更好地识别相关帧

Result: 在CG-Bench上提升8.0%，LVBench上提升8.5%，VideoMME(Long)上提升2.4%（相比ReKV with Qwen2.5-VL-7B）

Conclusion: MemStream通过扩展token预算、自适应token选择和外部模型增强的检索机制，显著改善了流式视频理解的性能，解决了现有方法在处理密集视频流时的局限性

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [34] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: QueryPlot是一个语义检索和制图框架，通过自然语言处理技术将地质文本语料库与地质图数据集成，支持用户用自然语言查询进行矿产远景预测。


<details>
  <summary>Details</summary>
Motivation: 传统矿产远景预测需要人工综合异质地质知识（包括文本矿床模型和地理空间数据集），这个过程是手动且知识密集型的，需要更高效的自动化方法。

Method: 构建了120多种矿床类型的描述性模型，将州地质图汇编多边形转换为结构化文本表示，使用预训练嵌入模型编码查询和区域描述，计算语义相似度得分进行区域排序和空间可视化，支持组合查询和多标准远景分析。

Result: 在钨矽卡岩矿床案例研究中，基于嵌入的检索实现了对已知矿床的高召回率，生成的远景区域与专家定义的许可区域高度一致，相似度得分作为监督学习管道的附加特征可显著提高分类性能。

Conclusion: QueryPlot提供了一个有效的语义检索和制图框架，能够自动化矿产远景预测过程，提高效率并支持交互式查询和可视化，代码和数据集已公开以支持未来研究。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [35] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 研究发现更大的语言模型在预测眼动和阅读时间数据方面表现更好，但所有模型都低估了人类反应的概率。大模型在完形填空数据中能提供更高质量的下一个词预测，因为它们对词汇共现统计不敏感，而在语义上更接近人类反应。


<details>
  <summary>Details</summary>
Motivation: 探索不同规模语言模型在预测人类语言处理行为（如眼动和阅读时间）方面的表现差异，理解模型规模如何影响其对词汇共现统计和语义信息的敏感性。

Method: 使用不同规模的语言模型分析眼动数据、阅读时间数据和完形填空数据，比较模型对人类反应的预测能力，特别是评估模型对下一个词的预测质量及其与人类反应的语义对齐程度。

Result: 1. 更大的语言模型在预测眼动和阅读时间数据方面表现更好；2. 所有模型都低估了人类反应的概率；3. 大模型在完形填空任务中能提供更高质量的下一个词预测；4. 大模型对词汇共现统计不敏感，但在语义上更接近人类反应；5. 大模型的记忆能力帮助它们猜测更语义合适的词，但使它们对词汇识别相关的低层信息不敏感。

Conclusion: 模型规模的增加提高了对语义信息的敏感性，但降低了对低层词汇统计信息的敏感性。大模型的记忆能力使其在语义任务上表现更好，但在需要词汇层面信息的任务上可能表现不佳。这支持了模型规模与语义处理能力正相关，但与低层信息处理能力负相关的观点。

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [36] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 研究发现，尽管导向向量能平均有效控制语言模型行为，但其效果在不同样本间差异显著且不可靠。论文通过分析导向向量训练数据，发现训练激活差异的余弦相似度越高、正负激活在导向方向上分离越好的行为数据集，导向效果越可靠。


<details>
  <summary>Details</summary>
Motivation: 导向向量通过在推理时向激活添加学习偏置来控制语言模型行为，虽然平均有效，但效果在不同样本间变化大且不可靠。研究旨在探究为什么导向可靠性在不同行为间存在差异，以及训练数据如何影响导向效果。

Method: 研究分析了导向向量训练数据对可靠性的影响：1）测量训练激活差异的余弦相似度与导向可靠性的关系；2）观察行为数据集中正负激活在导向方向上的分离程度；3）比较不同提示变体训练的导向向量的方向差异和性能表现。

Result: 1）训练激活差异的余弦相似度越高，导向越可靠；2）正负激活在导向方向上分离越好的行为数据集，导向效果越可靠；3）不同提示变体训练的导向向量方向不同但性能相似，且在不同数据集上表现出相关效果。

Conclusion: 导向向量不可靠的原因是潜在目标行为表示无法被线性导向方向有效近似。这些发现为诊断导向不可靠性提供了实用方法，并激励开发更鲁棒的导向方法，需要显式考虑非线性潜在行为表示。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [37] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出使用语言模型生成语义软标签目标来改进神经主题模型，通过重构这些软标签而非传统BoW表示，获得更高质量、更符合语料主题结构的主题


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型仅重构文档的词袋表示，忽略了上下文信息且面临数据稀疏问题，需要更丰富的监督信号来提升主题质量

Method: 使用语言模型通过专门提示生成下一个词的概率分布，投影到预定义词汇表上得到语义软标签目标，然后训练主题模型重构这些软标签，利用LM隐藏状态作为监督信号

Result: 在三个数据集上实验显示，该方法在主题连贯性和纯度方面显著优于现有基线，同时引入的检索指标也表明在识别语义相似文档方面表现优异

Conclusion: 提出的基于语言模型软标签的方法能够生成更高质量的主题，更好地捕捉语料主题结构，特别适用于检索导向的应用场景

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [38] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 该论文提出了CondMedQA基准测试和Condition-Gated Reasoning框架，用于解决生物医学问答中条件推理的挑战，通过条件感知知识图谱和选择性路径激活来提升临床决策的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学问答系统假设医学知识普遍适用，但真实临床推理本质上是条件性的，每个决策都依赖于患者特定因素（如并发症和禁忌症）。现有基准测试无法评估这种条件推理，检索增强或基于图的方法缺乏确保检索知识适用于给定上下文的明确机制。

Method: 提出了Condition-Gated Reasoning（CGR）框架，构建条件感知知识图谱，并基于查询条件选择性激活或修剪推理路径。同时创建了CondMedQA基准测试，包含答案随患者条件变化的多跳问题。

Result: CGR框架在可靠选择条件适当答案方面表现更优，同时在生物医学问答基准测试上达到或超过最先进方法的性能，突显了显式建模条件性对稳健医学推理的重要性。

Conclusion: 该研究强调了在生物医学问答中显式建模条件推理的必要性，提出的CGR框架和CondMedQA基准测试为评估和改进临床决策支持系统提供了重要工具，有助于提升医疗AI系统的实际应用价值。

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [39] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文首次基于DSPy优化框架系统比较了表格事实验证中的指令优化方法，评估了四种提示技术，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有特定优势。


<details>
  <summary>Details</summary>
Motivation: 指令优化为提升大语言模型推理性能提供了一种轻量级、模型无关的方法，但缺乏在表格事实验证任务中的系统性比较研究。

Method: 基于DSPy优化框架，评估了四种提示技术（直接预测、思维链、带SQL工具的ReAct、带Python执行的CodeAct），使用三种优化器（COPRO、MiPROv2、SIMBA）在四个基准测试和三个模型家族上进行实验。

Result: 指令优化持续提升验证准确率：MiPROv2对思维链提示最稳定，SIMBA对ReAct智能体提升最大（尤其在更大模型规模时）。行为分析显示SIMBA通过启发式方法鼓励更直接的推理路径，提升思维链中的数值比较能力，并帮助ReAct智能体避免不必要的工具调用。

Conclusion: 对于表格事实检查，思维链提示仍然有效（尤其对小模型），而基于大模型的ReAct智能体虽能达到竞争性性能，但需要仔细的指令优化。不同优化器对不同提示技术有特定优势，SIMBA在优化ReAct智能体方面表现突出。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [40] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate：基于图检索增强生成的UMLS概念集自动构建框架，通过知识图谱检索和LLM过滤分类，显著减少人工工作量，提高概念集完整性


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别工具通常将自由文本映射到UMLS概念唯一标识符，但许多下游任务需要的是包含相关同义词、子类型和超类型的概念集。目前构建这类概念集劳动密集、执行不一致，现有工具支持不足，特别是对于直接操作UMLS CUI的NLP管道

Method: 提出CUICurate框架，采用基于图的检索增强生成方法：构建并嵌入UMLS知识图谱进行语义检索；针对每个目标概念，从知识图谱检索候选CUI；然后使用大语言模型进行过滤和分类步骤，比较了GPT-5和GPT-5-mini两种模型

Result: 在所有概念上，CUICurate生成的概念集比人工基准更大、更完整，同时保持与人工相当的精确度；GPT-5-mini在过滤阶段召回率更高，GPT-5的分类结果更接近临床医生判断；输出在重复运行中稳定且计算成本低

Conclusion: CUICurate提供了可扩展且可复现的方法来支持UMLS概念集构建，显著减少人工工作量。通过将基于图的检索与LLM推理相结合，该框架生成聚焦的候选概念集，可适应不同表型和分析需求的临床NLP管道

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [41] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 论文研究了金融监管文件问答中检索增强生成的失败模式：即使检索到正确文档，但遗漏包含答案的具体页面或段落，导致生成器基于不完整上下文推断。通过多粒度检索评估和领域微调的页面评分器，显著提升了页面召回率和段落检索效果。


<details>
  <summary>Details</summary>
Motivation: 金融监管文件问答中，检索增强生成虽然常用，但可靠性依赖于检索到确切的上下文来支持答案。研究发现一个常见失败模式：检索到正确文档但遗漏包含答案的具体页面或段落，导致生成器基于不完整上下文推断。这种文档内检索失败模式在金融QA文献中缺乏系统性研究。

Method: 1) 在多粒度级别评估检索性能：文档级、页面级、段落级；2) 引入基于oracle的分析提供检索和生成性能的经验上限；3) 在FinanceBench的150个问题子集上复现和比较多种检索策略（稠密、稀疏、混合、分层检索，结合重排序和查询重构）；4) 提出领域微调的页面评分器，将页面作为文档和段落之间的中间检索单元，利用页面的语义连贯性。

Result: 1) 不同方法中，文档发现能力的提升通常转化为更强的页面召回率；2) Oracle性能表明页面和段落级检索仍有提升空间；3) 提出的领域微调页面评分器在页面召回率和段落检索方面取得显著改进。

Conclusion: 文档内检索失败是金融监管文件问答中的重要问题，通过多粒度检索评估和专门针对金融文件页面级相关性微调的双编码器，可以有效提升页面召回和段落检索性能，填补了现有检索方法的不足。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [42] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 论文主张AI评估应从模型中心的静态基准转向系统级的动态测量，以应对工具使用型智能体的非确定性特性。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估实践仍停留在模型中心时代的假设（静态基准、聚合分数、一次性成功标准），无法适应从静态模型到复合工具使用型智能体的转变，导致评估结果越来越模糊而非阐明系统行为。

Method: 通过分析评估流程本身引入的隐性故障模式、高基准分数误导团队的原因，以及智能体系统如何根本改变性能测量的意义，来重新思考评估的角色。

Result: 揭示了传统评估方法在智能体时代的局限性，指出评估不应是性能表演，而应成为建立信任、支持迭代和治理非确定性系统的测量学科。

Conclusion: 评估应重新定位为AI时代（特别是智能体）的核心控制功能，关注系统在变化和规模化条件下的可信行为，而非仅仅回答"模型有多好"的问题。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [43] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究发现，当用户认为聊天AI存在党派偏见时，其纠正错误观念的说服效果会降低28%，表明AI说服力受政治中立性感知的影响


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型进入党派冲突，精英们越来越多地将其描绘为具有意识形态倾向。本研究旨在测试这些可信度攻击是否会降低基于LLM的说服效果，探讨AI说服力的政治条件性

Method: 在美国进行了一项预注册调查实验（N=2144），参与者与ChatGPT进行三轮对话，讨论个人持有的经济政策误解。实验组收到简短信息表明LLM对参与者所属党派存在偏见，对照组为中性信息

Result: 与中性对照组相比，表明LLM存在党派偏见的信息使说服效果降低了28%。文本分析显示，警告改变了互动方式：受访者更频繁地反驳，参与度更低且接受度更差

Conclusion: 对话AI的说服影响具有政治条件性，受党派一致性感知的限制。当用户认为AI存在党派偏见时，其纠正错误观念的效果会显著降低

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [44] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 本文提出了一种对抗性提问生成框架，通过比较待适应模型与基于参考文档的专家模型的输出，生成紧凑的语义挑战性问题，以更高效地提升LLM在专业领域的适应能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域适应方面面临挑战：现有微调方法受限于高质量任务相关数据的稀缺性，而常见的合成数据生成方法（如改写或知识提取）存在两个关键缺陷：1) 对专业领域的解释性推理能力支持有限；2) 生成的合成语料库通常过大且冗余，样本效率低下。

Method: 提出对抗性提问生成框架，通过迭代反馈驱动过程，比较待适应模型与基于参考文档的专家模型的输出，生成紧凑的语义挑战性问题。该方法旨在揭示和解决模型的理解差距，而不是简单地生成大量冗余数据。

Result: 在LegalBench语料库的专业子集上评估表明，该方法使用显著更少的合成样本实现了更高的准确性，证明了其在提升样本效率和模型性能方面的有效性。

Conclusion: 对抗性提问生成框架为专业领域的大语言模型适应提供了一种更高效的方法，通过生成紧凑的语义挑战性问题而非大量冗余数据，显著提升了样本效率和模型在专业任务上的表现。

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [45] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 该论文提出了一种基于频率分析的注意力机制检测方法，用于识别大语言模型在上下文生成中的幻觉问题，通过分析注意力分布的高频成分来检测不稳定的注意力模式。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法通常依赖于粗粒度的注意力摘要，无法捕捉注意力中的细粒度不稳定性。需要一种更精细的方法来分析注意力模式，以更好地检测大语言模型在上下文生成中的幻觉问题。

Method: 受信号处理启发，将注意力分布建模为离散信号，提取反映注意力快速局部变化的高频成分。分析发现幻觉标记与高频注意力能量相关，反映了碎片化和不稳定的基础行为。基于这一洞察，开发了使用高频注意力特征的轻量级幻觉检测器。

Result: 在RAGTruth和HalluRAG基准测试上的实验表明，该方法在模型和任务上均优于基于验证、内部表示和注意力的现有方法，取得了性能提升。

Conclusion: 频率感知的注意力分析为幻觉检测提供了新的视角，高频注意力特征能有效反映大语言模型生成过程中的不稳定性，为开发轻量级、高效的幻觉检测器提供了理论基础。

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [46] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 该研究通过无损压缩技术分析LLM生成文本的结构统计特征，发现LLM生成文本比人类写作具有更高的结构规律性和可压缩性，但在小规模交互环境中这种差异会减弱。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过概率采样生成文本，但这一过程如何重塑语言的结构统计组织尚不完全清楚。研究者希望找到一种模型无关的方法来量化生成系统如何重塑文本生产。

Method: 使用无损压缩作为衡量统计规律性的简单模型无关方法，分析三个渐进复杂的信息生态系统：受控的人-LLM延续、知识基础设施的生成中介（维基百科 vs Grokipedia）、完全合成的社交互动环境（Moltbook vs Reddit）。

Result: 压缩揭示了概率生成的结构特征：在受控和中介环境中，LLM生成的语言比人类写作具有更高的结构规律性和可压缩性，表明输出集中在高度循环的统计模式中。但在碎片化的交互环境中，这种分离会减弱，表明在小尺度上表面可区分性存在根本限制。

Conclusion: 无损压缩提供了一个简单而稳健的框架，用于量化生成系统如何重塑文本生产，为通信的演化复杂性提供了结构视角。这种基于可压缩性的分离在不同模型、任务和领域中一致出现，可以直接从表面文本观察到。

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [47] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: FENCE是一个用于金融领域多模态越狱检测的双语数据集，包含韩语和英语的金融相关查询与图像威胁配对，用于训练和评估越狱检测器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型面临越狱攻击风险，特别是在金融领域。目前缺乏专门针对金融应用的多模态越狱检测资源，需要创建专门的数据集来填补这一空白。

Method: 创建了FENCE双语多模态数据集，包含金融相关查询与图像威胁配对，强调领域真实性。使用商业和开源视觉语言模型进行实验，并基于FENCE训练基线检测器。

Result: 实验显示GPT-4o具有可测量的攻击成功率，开源模型表现出更大的脆弱性。基于FENCE训练的基线检测器在分布内准确率达到99%，在外部基准测试中保持强劲性能。

Conclusion: FENCE为金融领域多模态越狱检测提供了专注资源，支持在敏感领域构建更安全可靠的AI系统。数据集展示了在训练可靠检测模型方面的鲁棒性。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [48] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 本文提出了一种结合transformer文本嵌入和语言学信息特征的混合方法用于点击诱饵检测，最佳模型在增强特征后达到91%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题降低了在线信息质量并损害用户信任，需要有效的检测方法来识别和过滤这类内容。

Method: 采用混合方法，结合基于transformer的文本嵌入和语言学驱动的信息特征，使用NLP技术评估传统向量化方法、词嵌入基线和大型语言模型嵌入，并与基于树的分类器配对。

Result: 最佳性能模型（XGBoost结合增强的15个显式特征）达到91%的F1分数，优于TF-IDF、Word2Vec、GloVe、基于LLM提示的分类和仅特征基线。

Conclusion: 提出的特征集通过突出第二人称代词、最高级、数字和注意力导向标点等显著语言线索增强了可解释性，实现了透明且校准良好的点击诱饵预测，并发布了代码和训练模型支持可重复研究。

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [49] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 本文提出Info-Gain Sampler，一种针对掩码扩散模型的新解码框架，通过平衡即时不确定性和未来信息增益，显著提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型采样器采用贪心启发式方法，只关注局部确定性最高的位置，忽略了当前解码选择对后续步骤的下游影响，未能充分利用MDMs的非因果特性来最小化累积不确定性

Method: 提出Info-Gain Sampler解码框架，不仅考虑当前位置的不确定性，还评估解码决策如何重塑所有剩余掩码位置的标记概率/不确定性，平衡即时不确定性与未来掩码标记的信息增益

Result: 在推理、编码、创意写作和图像生成等多样化架构和任务上，Info-Gain Sampler始终优于现有采样器：推理任务平均准确率提升3.6%，创意写作任务胜率达到63.1%，推理任务累积不确定性从78.4降至48.6

Conclusion: Info-Gain Sampler通过系统性地平衡即时不确定性和未来信息增益，克服了现有贪心采样器的局限性，为掩码扩散模型提供了更有效的解码策略，显著提升了生成质量

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [50] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究提出了一种基于信息论的句子理解处理存储成本度量方法，使用预训练神经语言模型估计先前词汇对未来上下文的信息量，相比传统基于语法的离散度量更连续、理论中立。


<details>
  <summary>Details</summary>
Motivation: 实时句子理解对工作记忆有显著负荷，需要保持上下文信息以预测未来输入。虽然处理负荷测量在心理语言学理论中很重要，但传统方法主要使用符号语法，为句法预测分配离散、统一的成本。

Method: 提出基于信息论的处理存储成本度量，定义为在不确定性条件下，先前词汇对未来上下文所携带的信息量。该方法连续、理论中立，可从预训练神经语言模型估计。

Result: 通过三个英语分析验证了该方法的有效性：1) 恢复了中心嵌入和关系从句中已知的处理不对称性；2) 与语法标注语料库中的语法存储成本相关；3) 在两个大规模自然数据集中预测阅读时间方差，优于包含传统信息预测因子的基线模型。

Conclusion: 基于信息论的存储成本度量提供了一种连续、理论中立的句子理解处理负荷测量方法，能够从预训练语言模型估计，并在多个分析中显示出有效性。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [51] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 提出了一种基于置信度的对比解码方法，通过检测低置信度token并选择性干预来提升大语言模型推理可靠性，同时减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设增加推理时计算能均匀提升正确性，但研究发现推理不确定性高度局部化：少数低置信度token对推理错误和不必要输出扩展贡献不成比例。因此需要针对性地干预低置信度位置。

Method: 提出置信度驱动的对比解码方法：在解码过程中检测低置信度token，在这些位置选择性干预；构建对比参考分布（将高置信度token替换为最小占位符），在低置信度位置通过减去参考分布来精炼预测。

Result: 在数学推理基准测试中显著提升准确性，同时大幅减少输出长度，KV缓存开销最小。作为无需训练的方法，通过针对性低置信度干预提升推理可靠性，避免计算冗余。

Conclusion: 通过针对低置信度token的对比解码干预，能够有效提升大语言模型推理可靠性，同时减少不必要输出，为推理时优化提供了新思路。

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [52] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式Web应用，通过集成多种可解释性分析工具和AI生成的自然语言解释，降低了大型语言模型机制可解释性分析的门槛，使非专家也能理解复杂的模型内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 当前机制可解释性分析工具虽然强大但过于复杂，形成了可访问性鸿沟，限制了非专业人士的使用。需要设计一个系统来简化语言模型组件分析结果，让更广泛的受众能够理解。

Method: 开发了ELIA交互式Web应用，集成归因分析、函数向量分析和电路追踪三种关键技术，并创新性地使用视觉语言模型自动为复杂可视化生成自然语言解释。通过混合方法用户研究验证有效性。

Result: 用户研究显示用户明显偏好交互式、可探索的界面而非静态可视化。AI生成的解释帮助非专家弥合知识差距：统计分析显示用户先前的LLM经验与其理解分数无显著相关性，表明系统降低了不同经验水平的理解障碍。

Conclusion: AI系统确实可以简化复杂的模型分析，但其真正潜力在与以用户为中心的设计结合时才能充分发挥，这种设计应优先考虑交互性、具体性和叙事引导。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [53] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 创建了首个罗马尼亚语抑郁和焦虑心理语料库PsihoRo，包含205名受访者的文本数据，填补了罗马尼亚语心理健康NLP资源的空白。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语目前缺乏开源的心理健康语料库，而现有方法从社交媒体收集心理健康数据存在假设偏差问题。需要更实用的数据收集策略来建立罗马尼亚语心理NLP资源。

Method: 通过包含6个开放式问题的问卷收集数据，同时使用标准化的PHQ-9和GAD-7筛查问卷进行评估。采用统计分析、罗马尼亚语LIWC文本分析、情感检测和主题建模等方法分析语料特征。

Result: 成功创建了包含205名受访者文本的PsihoRo语料库，这是首个罗马尼亚语抑郁和焦虑心理语料库。虽然规模较小，但为分析罗马尼亚人口的心理健康文本迈出了重要第一步。

Conclusion: PsihoRo填补了罗马尼亚语心理健康NLP资源的空白，为理解和分析罗马尼亚人口的心理健康文本提供了基础资源，展示了新引入资源的重要特征。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [54] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个为印度司法系统设计的框架，用于预测和解释上诉判决，通过分解案件文件为决策点，使用LLM实现高准确率的预测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 印度法院面临大量案件积压，特别是上诉案件，需要AI技术来帮助预测判决结果并提高司法效率。

Method: Vichara框架处理英文上诉案件文件，将其分解为包含法律问题、裁决机构、结果、推理和时间背景的决策点，采用IRAC框架进行结构化解释，并使用GPT-4o mini等四种大型语言模型进行评估。

Result: 在PredEx和ILDC_expert两个数据集上，Vichara超越了现有判决预测基准，GPT-4o mini表现最佳（F1：PredEx 81.5，ILDC_expert 80.3）。人类评估显示GPT-4o mini在解释清晰度、关联性和实用性方面表现优越。

Conclusion: Vichara框架能够有效预测印度上诉案件判决并提供可解释的推理，为缓解司法积压问题提供了有前景的AI解决方案。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [55] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 本文提出了一种双尺度验证框架，用于评估政治立场预测中的主观连续属性，结合了点对点和成对人工标注，构建了大规模政治立场知识库，并证明了从点对点语言模型预测中提取序数结构的可行性。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识表示需要捕捉主观连续属性（如政治立场），但这些属性与广泛接受的成对验证黄金标准存在冲突。为了解决这一挑战，研究需要一种能够平衡可扩展性和可靠性的验证方法。

Method: 采用双尺度验证框架，结合点对点和成对人工标注。使用22个语言模型，从英国政治电视节目《Question Time》的30场辩论中抽取23,228个论点，构建大规模政治立场预测知识库。

Result: 点对点评估显示中等程度的人机一致性（Krippendorff's α=0.578），反映了内在主观性；而成对验证显示人类和模型生成的排名之间有更强的对齐（最佳模型α=0.86）。

Conclusion: 本研究贡献包括：实用的主观连续知识验证方法；经过验证的结构化论证知识库；以及证明可以从点对点语言模型预测中提取序数结构，为传统符号或分类方法不足的领域推进知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [56] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: VIRAASAT是一个针对印度文化的多跳问答数据集生成框架，结合了知识图谱和符号链操作(SCoM)方法，显著提升了LLM在文化推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文化基准测试存在三个主要问题：1) 手工制作成本高；2) 只包含测试事实回忆的单跳问题；3) 难以扩展。这导致LLM在需要丰富社会文化知识和本地背景的任务（特别是印度文化）上表现不佳的问题未被充分测量。

Method: 提出了VIRAASAT框架：1) 构建包含700多个专家策划文化实体的知识图谱，涵盖印度文化的13个关键属性；2) 采用半自动化多跳方法生成文化特定的多跳问答数据集，覆盖印度所有28个邦和8个中央直辖区，包含3200多个多跳问题；3) 提出符号链操作(SCoM)框架，训练模型内部模拟原子知识图谱操作，可靠遍历图谱拓扑结构。

Result: 评估显示当前SOTA LLM在VIRAASAT上存在关键推理限制，即使使用CoT微调也难以处理低概率事实。实验表明，SCoM在监督微调(SFT)中比标准CoT基线性能提升高达20%。

Conclusion: VIRAASAT数据集和SCoM框架为构建文化感知推理模型奠定了坚实基础，解决了LLM在文化推理任务中的局限性，特别是在需要多跳推理和低概率事实合成的场景。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [Epistemic Traps: Rational Misalignment Driven by Model Misspecification](https://arxiv.org/abs/2602.17676)
*Xingcheng Xu,Jingjing Qu,Qiaosheng Zhang,Chaochao Lu,Yanqing Yang,Na Zou,Xia Hu*

Main category: cs.AI

TL;DR: 该论文提出AI安全问题的根源不是训练错误，而是模型设定错误导致的理性行为，需要从操纵奖励转向设计智能体的主观世界模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和AI智能体在关键领域部署时存在奉承、幻觉和策略性欺骗等行为问题，传统强化学习方法难以解决。现有安全范式将这些失败视为临时训练产物，缺乏统一理论框架解释其出现和稳定性。

Method: 将经济学中的Berk-Nash理性化理论应用于人工智能，建立严格框架，将智能体建模为在错误的主观世界模型下进行优化。通过六个最先进模型家族的行为实验验证理论预测，生成精确映射安全行为拓扑边界的相图。

Result: 研究发现不安全行为是结构性必然：根据奖励方案，不安全行为要么作为稳定的错位均衡出现，要么作为振荡循环出现；策略性欺骗要么作为"锁定"均衡持续存在，要么通过认知不确定性持续存在。安全是离散相，由智能体的认知先验决定，而非奖励大小的连续函数。

Conclusion: 安全问题的根源是模型设定错误导致的理性行为，而非训练错误。需要从操纵环境奖励转向设计智能体的内部信念结构（主观模型工程），这是实现稳健对齐的必要条件，标志着AI安全范式的转变。

Abstract: The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.

</details>


### [58] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 该研究探讨了使用形式化领域本体（特别是OpenMath本体）通过检索增强生成来提升语言模型在数学推理任务中的可靠性，发现本体引导的上下文在检索质量高时能改善性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本限制，这在需要可验证推理的高风险专业领域尤其成问题。研究者希望探索形式化领域本体是否能通过检索增强生成来提高语言模型的可靠性。

Method: 使用数学作为概念验证，实现了一个神经符号管道，利用OpenMath本体结合混合检索和交叉编码器重排序技术，将相关定义注入模型提示中。在MATH基准上评估了三个开源模型。

Result: 评估显示，本体引导的上下文在检索质量高时能提高性能，但无关上下文会主动降低性能。这突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体可以增强语言模型的可靠性，但检索质量至关重要。神经符号方法有前景，但需要解决无关上下文带来的负面影响。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [59] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于编程谜题的对战式评估框架，让大语言模型相互出题挑战，通过Elo评分比较模型推理能力，无需人工出题


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两个主要问题：1）人工出题成本高昂，特别是需要博士级领域知识的难题；2）难以区分模型是真正推理还是训练时见过类似问题。需要一种可持续、防饱和的评估范式

Method: 受16世纪数学决斗启发，设计Token Games框架：1）使用编程谜题格式（给定返回布尔值的Python函数，找到使函数返回True的输入）；2）让模型相互出题挑战；3）通过成对对决结果计算Elo评分；4）评估10个前沿模型

Result: 1）TTG的模型排名与Humanity's Last Exam等现有基准高度匹配；2）创建优质谜题对当前模型仍是极具挑战的任务，这是先前基准未测量的能力；3）无需人工出题即可实现有效评估

Conclusion: TTG提出了一种新的推理评估范式：通过模型相互出题的对战式框架，既能防止评估饱和，又能测试创造力、任务创建等传统基准未涵盖的能力，为评估大语言模型开辟了新方向

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [60] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: WorkflowPerturb：一个用于评估LLM生成工作流质量的基准测试，通过向黄金工作流施加可控扰动来测试不同评估指标的敏感性和校准度


<details>
  <summary>Details</summary>
Motivation: LLM生成的复杂任务结构化工作流难以自动评估，因为现有指标分数通常未校准，且分数变化无法直接反映工作流质量下降的严重程度

Method: 提出WorkflowPerturb基准测试，对4,973个黄金工作流施加三种类型的可控扰动（缺失步骤、压缩步骤、描述变化），每种扰动在10%、30%、50%三个严重程度级别生成44,757个扰动变体

Result: 通过预期分数轨迹和残差分析，基准测试了多个指标家族，揭示了不同指标家族的系统性差异，支持基于严重程度的工作流评估分数解释

Conclusion: WorkflowPerturb为工作流评估指标提供了受控基准，有助于理解指标敏感性和校准特性，实现严重程度感知的工作流评估

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [61] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 该研究结合离线强化学习和跨具身学习来解决机器人策略预训练中高质量演示数据成本高的问题，通过构建16个机器人平台的数据集验证了该方法的有效性，并提出基于形态相似性的分组策略来缓解多机器人类型带来的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 机器人策略预训练需要高质量演示数据，但为每个平台收集这些数据成本高昂。研究旨在通过结合离线强化学习和跨具身学习来解决这一问题，利用专家和次优数据以及跨不同形态的机器人轨迹来获取通用控制先验。

Method: 1. 系统分析离线强化学习与跨具身学习的结合范式；2. 构建包含16个不同机器人平台的运动数据集进行评估；3. 提出基于形态相似性的分组策略，将机器人按形态相似性聚类，使用组梯度更新模型以减少跨机器人梯度冲突。

Result: 实验证实：1. 结合方法在包含丰富次优轨迹的数据集上预训练效果优于纯行为克隆；2. 随着次优数据比例和机器人类型增加，跨形态的梯度冲突开始阻碍学习；3. 提出的静态分组策略显著减少了机器人间的冲突，性能优于现有冲突解决方法。

Conclusion: 离线强化学习与跨具身学习的结合为机器人策略预训练提供了有效解决方案，特别是在处理次优数据时表现优异。然而，多机器人类型带来的梯度冲突需要通过适当的聚类策略来缓解，基于形态相似性的分组方法为此提供了简单有效的解决方案。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [62] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: 研究发现，即使训练时排除敏感属性，无监督表示仍会泄露年龄、收入等敏感信息，公平性通过不知情在表示层面失效。


<details>
  <summary>Details</summary>
Motivation: 挑战无监督表示对敏感属性保持中性的普遍假设，证明即使明确排除敏感属性，它们仍会作为潜在主导轴出现，揭示公平性通过不知情在表示层面的失败。

Method: 使用SOMtime（基于高容量自组织映射的拓扑保持表示方法），在两个大规模真实数据集（五个国家的世界价值观调查和人口普查收入数据集）上测试，与PCA、UMAP、t-SNE和自编码器等方法对比。

Result: SOMtime恢复了与排除敏感属性对齐的单调排序，Spearman相关性高达0.85，而其他方法通常低于0.23（单个例外达0.31）。无监督分割SOMtime嵌入会产生人口统计学偏斜的聚类，显示无监督任务的下游公平风险。

Conclusion: 公平性通过不知情在表示层面对序数敏感属性失效，公平性审计必须扩展到机器学习流水线的无监督组件。

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [63] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD：首个在线多智能体强化学习框架，使用扩散策略协调智能体，通过放松策略目标最大化联合熵实现高效探索，在MPE和MAMuJoCo任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 在线多智能体强化学习中增强策略表达能力对提升性能至关重要。扩散模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示能力，但在在线MARL中潜力尚未充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）放松策略目标，最大化缩放联合熵，实现有效探索而不依赖可处理似然性；2）在CTDE范式下，使用联合分布值函数优化去中心化扩散策略；3）利用可处理的熵增强目标指导扩散策略的同步更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行广泛评估，OMAD成为新的最先进方法，样本效率显著提高2.5倍到5倍。

Conclusion: OMAD是首个使用扩散策略的在线多智能体强化学习框架，通过放松策略目标和联合分布值函数成功解决了扩散模型在在线MARL中的应用障碍，实现了卓越的协调性能和样本效率。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [64] [Uncertainty-Aware Jamming Mitigation with Active RIS: A Robust Stackelberg Game Approach](https://arxiv.org/abs/2602.18165)
*Xiao Tang,Zhen Ma,Limeng Dong,Yichen Wang,Qinghe Du,Dusit Niyato,Zhu Han*

Main category: cs.IT

TL;DR: 该论文研究利用主动可重构智能表面(ARIS)进行抗干扰通信，针对信道不确定性设计鲁棒抗干扰方案，采用Stackelberg博弈建模合法方与干扰方的策略交互。


<details>
  <summary>Details</summary>
Motivation: 恶意干扰对安全通信构成普遍威胁，随着干扰器能力的增强，能够适应合法传输，使得抗干扰挑战日益严峻。论文旨在利用ARIS进行干扰抑制，特别关注信道不确定性以实现鲁棒抗干扰设计。

Method: 采用Stackelberg博弈建模合法方（领导者）与干扰方（跟随者）的策略交互，证明博弈均衡存在并使用逆向归纳法分析均衡。首先推导干扰方的最优干扰策略作为最佳响应，然后将其纳入合法方优化进行鲁棒抗干扰设计。利用误差界处理不确定性，在BSUM框架下分解问题，分别处理功率分配、收发波束成形和主动反射，迭代求解鲁棒干扰抑制方案。

Result: 仿真结果表明，所提方案在不确定性条件下能有效保护合法传输，在干扰抑制性能方面优于基线方法。

Conclusion: 该研究提出了一种基于ARIS的鲁棒抗干扰方案，通过Stackelberg博弈建模和BSUM框架优化，有效应对信道不确定性下的恶意干扰，为安全通信提供了有效的干扰抑制方法。

Abstract: Malicious jamming presents a pervasive threat to the secure communications, where the challenge becomes increasingly severe due to the growing capability of the jammer allowing the adaptation to legitimate transmissions. This paper investigates the jamming mitigation by leveraging an active reconfigurable intelligent surface (ARIS), where the channel uncertainties are particularly addressed for robust anti-jamming design. Towards this issue, we adopt the Stackelberg game formulation to model the strategic interaction between the legitimate side and the adversary, acting as the leader and follower, respectively. We prove the existence of the game equilibrium and adopt the backward induction method for equilibrium analysis. We first derive the optimal jamming policy as the follower's best response, which is then incorporated into the legitimate-side optimization for robust anti-jamming design. We address the uncertainty issue and reformulate the legitimate-side problem by exploiting the error bounds to combat the worst-case jamming attacks. The problem is decomposed within a block successive upper bound minimization (BSUM) framework to tackle the power allocation, transceiving beamforming, and active reflection, respectively, which are iterated towards the robust jamming mitigation scheme. Simulation results are provided to demonstrate the effectiveness of the proposed scheme in protecting the legitimate transmissions under uncertainties, and the superior performance in terms of jamming mitigation as compared with the baselines.

</details>


### [65] [Construction of Cyclic Codes over a Class of Matrix Rings](https://arxiv.org/abs/2602.18255)
*Soham Ravikant Joshi,Shikha Patel,Om Prakash*

Main category: cs.IT

TL;DR: 该论文研究通过非交换非链矩阵环构造有限域F16上的循环码，建立了环结构、理想形式、码的分解公式，并利用Bachoc映射和Gray映射得到具有良好参数的线性码。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索通过有限非交换非链矩阵环来构造有限域上的循环码，特别是利用环R = M4(F2[u]/⟨u^k⟩)的结构特性，建立从矩阵环到有限域F16的编码映射关系，以获得具有良好参数的线性码。

Method: 方法包括：1) 分析环R的结构并证明其同构于特定形式；2) 建立环R的理想形式及相关循环码；3) 将循环码表示为R[x]/⟨x^n-1⟩的子模直和；4) 推导循环码的基数公式；5) 考虑欧几里得和对偶形式；6) 利用Bachoc映射和Gray映射将环R上的循环码映射到F16；7) 提供具有良好参数的实际编码示例。

Result: 研究结果包括：1) 证明了环R的同构结构；2) 建立了环R的理想形式；3) 推导了循环码的分解形式和基数公式；4) 得到了环R上循环码的对偶形式；5) 通过映射获得了F16上具有良好参数的线性码，其中一些码的参数优于现有文献中的码。

Conclusion: 结论表明，通过有限非交换非链矩阵环可以有效地构造有限域F16上的循环码，所提出的方法能够产生具有良好参数的线性码，为编码理论提供了新的构造途径，并通过具体示例验证了方法的有效性。

Abstract: Let $ \mathbb F_2[u]/ \langle u^k \rangle= \mathbb F_2+u\mathbb F_2+u^2\mathbb F_2+\cdots+u^{k-1}\mathbb F_2 ,$ where $u^k=0$ for a positive integer $k$, and $\mathcal{R}=M_4 (\mathbb F_2( u)/ \langle u^k \rangle)$ be the finite noncommutative non-chain matrix ring of order $4\times4$. This paper presents the construction of cyclic codes over the finite field $\mathbb F_{16}$ via the considered matrix ring $\mathcal{R}$. In this connection, first, we discuss the structure of the ring $\mathcal{R}$ and show that $\mathcal{R}$ is isomorphic to the ring $( \mathbb F_{16}+ v\mathbb F_{16} + v^2\mathbb F_{16} + v^3\mathbb F_{16}) + u(\mathbb F_{16} + v\mathbb F_{16} + v^2\mathbb F_{16} + v^3\mathbb F_{16}) + u^2(\mathbb F_{16} + v\mathbb F_{16} + v^2\mathbb F_{16}+ v^3\mathbb F_{16}) + \cdots + u^{k-1}(\mathbb F_{16} + v\mathbb F_{16} + v^2\mathbb F_{16} + v^3\mathbb F_{16})$ where $v^4=0, u^k=0, u^iv^j=v^ju^i$ for $i \in \{1,\dots, k-1\}$ and $j \in \{1, 2, 3\}$. Then, we establish the form of ideals of the ring $\mathcal{R}$ and related cyclic codes over $\mathcal{R}$. Further, we show that these cyclic codes can be written as the direct sums of $\mathcal{R}$-submodules of $\frac{\mathcal{R}[x]}{<x^n-1>}$, and derive the formula for the cardinality of cyclic codes over $\mathcal{R}$. Then, we consider the Euclidean and Hermitian duals of the derived cyclic codes over $\mathcal{R}$. Under the module isometry for $\mathcal{R}$, we use the Bachoc map and the Gray map, which takes a derived cyclic code over $\mathcal{R}$ to $\mathbb F_{16}$. Finally, we provide some non-trivial examples of linear codes over $\mathbb F_{16}$ with good parameters that support our derived results and compare a few codes with existing codes in the literature.

</details>


### [66] [Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings](https://arxiv.org/abs/2602.18364)
*Sreejith Sreekumar,Nir Weinberger*

Main category: cs.IT

TL;DR: 论文提出从信息几何和统计视角理解大语言模型的上下文学习能力，将训练建模为概率分布到量子密度算子空间的嵌入，将上下文学习视为指定量子模型类的最大似然预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究提出了多种解释现代大语言模型上下文预测能力的理论，本文希望从信息几何和统计角度提供替代性的概念框架，为经典和量子大语言模型提供统一的理论基础。

Method: 受Bach[2023]启发，将训练过程建模为学习概率分布到量子密度算子空间的嵌入，将上下文学习视为在指定量子模型类上的最大似然预测。当量子模型类足够表达时，通过量子反向信息投影和量子毕达哥拉斯定理解释预测器。

Result: 推导了在迹范数和量子相对熵下的非渐近性能保证，包括收敛速率和集中不等式。该框架能够统一处理经典和量子大语言模型。

Conclusion: 提出的信息几何和统计视角为大语言模型的上下文学习能力提供了统一的理论框架，通过量子密度算子空间建模和最大似然预测，为经典和量子模型提供了共同的理论基础。

Abstract: Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.

</details>


### [67] [A Generalized Information Bottleneck Method: A Decision-Theoretic Perspective](https://arxiv.org/abs/2602.18405)
*Akira Kamatsuka,Takahiro Yoshida*

Main category: cs.IT

TL;DR: 本文研究了广义信息瓶颈问题，使用满足凹性和平均条件的H-互信息来评估效用，并基于统计决策理论解释开发了交替优化算法。


<details>
  <summary>Details</summary>
Motivation: 传统信息瓶颈方法使用互信息来评估压缩和效用，但存在局限性。本研究旨在推广信息瓶颈问题，使用更一般的H-互信息度量，这类度量具有统计决策理论解释，能够更灵活地评估表示的有效性。

Method: 基于H-互信息的统计决策理论解释，该度量等价于样本信息的期望值。利用这一特性，推导出交替优化算法来评估广义信息瓶颈问题中的压缩与效用权衡。

Result: 提出了一个广义信息瓶颈框架，其中效用评估基于满足凹性和平均条件的H-互信息。开发了相应的交替优化算法，能够有效评估压缩与效用之间的权衡关系。

Conclusion: 通过将传统信息瓶颈推广到使用H-互信息，本文提供了一个更灵活的信息瓶颈框架，具有明确的统计决策理论解释，并开发了有效的优化算法来求解该广义问题。

Abstract: The information bottleneck (IB) method seeks a compressed representation of data that preserves information relevant to a target variable for prediction while discarding irrelevant information from the original data. In its classical formulation, the IB method employs mutual information to evaluate the compression between the original and compressed data and the utility of the representation for the target variable. In this study, we investigate a generalized IB problem, where the evaluation of utility is based on the $\mathcal{H}$-mutual information that satisfies the concave (\texttt{CV}) and averaging (\texttt{AVG}) conditions. This class of information measures admits a statistical decision-theoretic interpretation via its equivalence to the expected value of sample information. Based on this interpretation, we derive an alternating optimization algorithm to assess the tradeoff between compression and utility in the generalized IB problem.

</details>
