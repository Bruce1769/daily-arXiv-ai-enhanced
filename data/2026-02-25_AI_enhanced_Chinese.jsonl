{"id": "2602.20635", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20635", "abs": "https://arxiv.org/abs/2602.20635", "authors": ["Ken Nakamura", "Takayuki Nozaki"], "title": "Insertion Correcting Capability for Quantum Deletion-Correcting Codes", "comment": "9 pages, submitted to Physical Review A", "summary": "This paper proves that any quantum t-deletion-correcting codes also correct a total of t insertion and deletion errors under a certain condition. Here, this condition is that a set of quantum states is defined as a quantum error-correcting code if the error spheres of its states are disjoint, as classical coding theory. In addition, this paper proposes the quantum indel distance and describes insertion and deletion errors correcting capability of quantum codes by this distance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u91cf\u5b50t\u5220\u9664\u7ea0\u9519\u7801\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u80fd\u7ea0\u6b63\u603b\u5171t\u4e2a\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5b50\u63d2\u5165\u5220\u9664\u8ddd\u79bb\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u91cf\u5b50\u7801\u7684\u63d2\u5165\u5220\u9664\u7ea0\u9519\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u7801\u5728\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\u4e0b\u7684\u7ea0\u9519\u80fd\u529b\uff0c\u5c06\u7ecf\u5178\u7f16\u7801\u7406\u8bba\u4e2d\u7684\u6982\u5ff5\u6269\u5c55\u5230\u91cf\u5b50\u9886\u57df\uff0c\u4e3a\u91cf\u5b50\u901a\u4fe1\u548c\u5b58\u50a8\u4e2d\u7684\u5b9e\u9645\u9519\u8bef\u7c7b\u578b\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "1. \u8bc1\u660e\u91cf\u5b50t\u5220\u9664\u7ea0\u9519\u7801\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u80fd\u7ea0\u6b63\u603b\u5171t\u4e2a\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\uff1b2. \u5b9a\u4e49\u91cf\u5b50\u63d2\u5165\u5220\u9664\u8ddd\u79bb\uff1b3. \u4f7f\u7528\u7c7b\u4f3c\u7ecf\u5178\u7f16\u7801\u7406\u8bba\u4e2d\u7684\u9519\u8bef\u7403\u4e0d\u76f8\u4ea4\u6761\u4ef6\u6765\u5b9a\u4e49\u91cf\u5b50\u7ea0\u9519\u7801\u3002", "result": "\u5efa\u7acb\u4e86\u91cf\u5b50\u7801\u5728\u63d2\u5165\u548c\u5220\u9664\u9519\u8bef\u4e0b\u7684\u7ea0\u9519\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5220\u9664\u7ea0\u9519\u7801\u4e0e\u63d2\u5165\u5220\u9664\u7ea0\u9519\u7801\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5316\u7ea0\u9519\u80fd\u529b\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "conclusion": "\u91cf\u5b50\u63d2\u5165\u5220\u9664\u7ea0\u9519\u7801\u7406\u8bba\u4e3a\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5c06\u7ecf\u5178\u7f16\u7801\u7406\u8bba\u4e2d\u7684\u63d2\u5165\u5220\u9664\u7ea0\u9519\u6982\u5ff5\u6210\u529f\u6269\u5c55\u5230\u91cf\u5b50\u9886\u57df\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u610f\u4e49\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "\u63d0\u51faDMEMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u673a\u5236\u6765\u8c03\u5236\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edf\u6269\u6563\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u751f\u6210\u8f68\u8ff9\u65f6\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u73af\u5883\u673a\u5236\u7684\u4e00\u81f4\u6027\u8981\u6c42\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8f68\u8ff9\u4e0e\u771f\u5b9e\u73af\u5883\u673a\u5236\u5b58\u5728\u663e\u8457\u5dee\u5f02", "method": "\u63d0\u51faDMEMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u673a\u5236\uff08\u7279\u522b\u662f\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDMEMM\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5c06\u73af\u5883\u673a\u5236\u5efa\u6a21\u878d\u5165\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0cDMEMM\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u771f\u5b9e\u73af\u5883\u4e00\u81f4\u6027\u7684\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u6548\u679c"}}
{"id": "2602.20351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20351", "abs": "https://arxiv.org/abs/2602.20351", "authors": ["Aleksandr Gushchin", "Dmitriy S. Vatolin", "Anastasia Antsiferova"], "title": "BiRQA: Bidirectional Robust Quality Assessment for Images", "comment": null, "summary": "Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean \"anchor\" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.", "AI": {"tldr": "BiRQA\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5411\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u5904\u7406\u56db\u4e2a\u5feb\u901f\u4e92\u8865\u7279\u5f81\uff0c\u901a\u8fc7\u951a\u5b9a\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u901f\u5ea6\u6162\u4e14\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6270\u52a8\u653b\u51fb\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u51c6\u786e\u53c8\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u8bc4\u4f30\u6a21\u578b\u3002", "method": "1. \u53cc\u5411\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u67b6\u6784\uff1a\u5904\u7406\u56db\u4e2a\u5feb\u901f\u4e92\u8865\u7279\u5f81\uff1b2. \u81ea\u5e95\u5411\u4e0a\u6ce8\u610f\u529b\u6a21\u5757\uff1a\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u95e8\u5c06\u7ec6\u7c92\u5ea6\u7ebf\u7d22\u6ce8\u5165\u7c97\u7c92\u5ea6\u5c42\u7ea7\uff1b3. \u81ea\u9876\u5411\u4e0b\u4ea4\u53c9\u95e8\u63a7\u5757\uff1a\u5c06\u8bed\u4e49\u4e0a\u4e0b\u6587\u8def\u7531\u56de\u9ad8\u5206\u8fa8\u7387\uff1b4. \u951a\u5b9a\u5bf9\u6297\u8bad\u7ec3\uff1a\u4f7f\u7528\u5e72\u51c0\"\u951a\u70b9\"\u6837\u672c\u548c\u6392\u5e8f\u635f\u5931\u6765\u9650\u5236\u653b\u51fb\u4e0b\u7684\u9010\u70b9\u9884\u6d4b\u8bef\u5dee\u3002", "result": "1. \u5728\u4e94\u4e2a\u516c\u5171FR IQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6216\u5339\u914d\u5148\u524dSOTA\uff1b2. \u8fd0\u884c\u901f\u5ea6\u6bd4\u5148\u524dSOTA\u6a21\u578b\u5feb\u7ea63\u500d\uff1b3. \u5728\u672a\u89c1\u8fc7\u7684\u767d\u76d2\u653b\u51fb\u4e0b\uff0cKADID-10k\u4e0a\u7684SROCC\u4ece0.30-0.57\u63d0\u5347\u52300.60-0.84\uff1b4. \u7ed3\u5408\u4e86\u7ade\u4e89\u6027\u7cbe\u5ea6\u3001\u5b9e\u65f6\u541e\u5410\u91cf\u548c\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "conclusion": "BiRQA\u662f\u9996\u4e2a\u540c\u65f6\u5177\u5907\u7ade\u4e89\u6027\u7cbe\u5ea6\u3001\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u548c\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u4e3a\u56fe\u50cf\u538b\u7f29\u3001\u4fee\u590d\u548c\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.20300", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u67e5\u8be2\u8bed\u53e5\u7684\u7279\u5b9a\u7ed3\u6784\u7279\u5f81\uff08\u5982\u4ece\u53e5\u5d4c\u5957\u6df1\u5ea6\u3001\u6307\u4ee3\u4e0d\u660e\u786e\u7b49\uff09\u4e0eLLM\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\uff0c\u4e3a\u67e5\u8be2\u91cd\u5199\u548c\u5e72\u9884\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u4e0aLLM\u5e7b\u89c9\u88ab\u89c6\u4e3a\u6a21\u578b\u6216\u89e3\u7801\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u4f46\u672c\u6587\u4ece\u8bed\u8a00\u5b66\u89d2\u5ea6\u51fa\u53d1\uff0c\u8ba4\u4e3a\u67e5\u8be2\u8bed\u53e5\u7684\u5f62\u5f0f\u4e5f\u4f1a\u5f71\u54cd\u6a21\u578b\u54cd\u5e94\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7279\u5b9a\u7c7b\u578b\u7684\u67e5\u8be2\u662f\u5426\u66f4\u5bb9\u6613\u5f15\u53d1\u5e7b\u89c9\u3002", "method": "\u6784\u5efa22\u7ef4\u67e5\u8be2\u7279\u5f81\u5411\u91cf\uff0c\u6db5\u76d6\u4ece\u53e5\u590d\u6742\u5ea6\u3001\u8bcd\u6c47\u7a00\u6709\u5ea6\u3001\u6307\u4ee3\u3001\u5426\u5b9a\u3001\u53ef\u56de\u7b54\u6027\u548c\u610f\u56fe\u660e\u786e\u6027\u7b49\u8bed\u8a00\u5b66\u7279\u5f81\u3002\u4f7f\u7528369,837\u4e2a\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\uff0c\u8bc6\u522b\u4e0e\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\u7684\u67e5\u8be2\u7279\u5f81\u6a21\u5f0f\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u7a33\u5b9a\u7684\"\u98ce\u9669\u56fe\u8c31\"\uff1a\u6df1\u5ea6\u4ece\u53e5\u5d4c\u5957\u548c\u6307\u4ee3\u4e0d\u660e\u786e\u7b49\u7279\u5f81\u4e0e\u66f4\u9ad8\u7684\u5e7b\u89c9\u503e\u5411\u76f8\u5173\uff1b\u800c\u610f\u56fe\u660e\u786e\u548c\u53ef\u56de\u7b54\u6027\u5219\u4e0e\u8f83\u4f4e\u7684\u5e7b\u89c9\u7387\u76f8\u5173\u3002\u5176\u4ed6\u7279\u5f81\u5982\u9886\u57df\u7279\u5f02\u6027\u5219\u5448\u73b0\u6df7\u5408\u3001\u4f9d\u8d56\u6570\u636e\u96c6\u548c\u6a21\u578b\u7684\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u4e0e\u5e7b\u89c9\u98ce\u9669\u76f8\u5173\u7684\u53ef\u89c2\u6d4b\u67e5\u8be2\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u5f15\u5bfc\u6027\u67e5\u8be2\u91cd\u5199\u548c\u672a\u6765\u5e72\u9884\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8868\u660e\u901a\u8fc7\u4f18\u5316\u67e5\u8be2\u7ed3\u6784\u53ef\u4ee5\u964d\u4f4eLLM\u5e7b\u89c9\u98ce\u9669\u3002"}}
{"id": "2602.20354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "AI": {"tldr": "3DSPA\u662f\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u89c6\u9891\u771f\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\u6574\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u65e0\u9700\u53c2\u8003\u89c6\u9891\u5373\u53ef\u8bc4\u4f30\u89c6\u9891\u7684\u771f\u5b9e\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524dAI\u89c6\u9891\u751f\u6210\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u6709\u9650\u8303\u56f4\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u3001\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa3DSPA\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a3D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\uff0c\u5c063D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548cDINO\u8bed\u4e49\u7279\u5f81\u6574\u5408\u5230\u7edf\u4e00\u7684\u89c6\u9891\u8868\u793a\u4e2d\uff0c\u5efa\u6a21\u7269\u4f53\u8fd0\u52a8\u548c\u573a\u666f\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e3DSPA\u80fd\u53ef\u9760\u8bc6\u522b\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u89c6\u9891\uff0c\u5bf9\u8fd0\u52a8\u4f2a\u5f71\u66f4\u654f\u611f\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u5bf9\u89c6\u9891\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "\u5c063D\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u57fa\u4e8e\u8f68\u8ff9\u7684\u8868\u793a\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\uff0c\u5e76\u80fd\u9690\u5f0f\u6355\u6349\u7269\u7406\u89c4\u5219\u8fdd\u53cd\u3002"}}
{"id": "2602.20332", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "QueryBandits\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\uff0c\u51cf\u5c11\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u68af\u5ea6\u8c03\u6574\u3002", "motivation": "\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u6784\u90e8\u7f72\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u73b0\u6709\u7f13\u89e3\u5e7b\u89c9\u7684\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u4e8b\u540e\u68c0\u6d4b\u548c\u53c2\u6570\u7f16\u8f91\uff0c\u7f3a\u4e4f\u5bf9\u95ed\u6e90\u6a21\u578b\u7684\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u63d0\u51faQueryBandits\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u548c\u6821\u51c6\u7684\u5956\u52b1\u51fd\u6570\u5728\u7ebf\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u91cd\u5199\u7b56\u7565\uff08\u5982\u6539\u5199\u3001\u6269\u5c55\u7b49\uff09\u3002", "result": "\u572816\u4e2aQA\u573a\u666f\u4e2d\uff0c\u6700\u4f73QueryBandits\uff08Thompson Sampling\uff09\u76f8\u6bd4\u65e0\u91cd\u5199\u57fa\u7ebf\u83b7\u5f9787.5%\u7684\u80dc\u7387\uff0c\u5206\u522b\u6bd4\u96f6\u6837\u672c\u9759\u6001\u7b56\u7565\uff08\u6539\u5199\u548c\u6269\u5c55\uff09\u9ad8\u51fa42.6%\u548c60.3%\u3002\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u666e\u901a\u8001\u864e\u673a\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u7684\u91cd\u5199\u7b56\u7565\u9002\u7528\u4e8e\u6240\u6709\u67e5\u8be2\uff0c\u50f5\u5316\u7684\u91cd\u5199\u7b56\u7565\u53ef\u80fd\u52a0\u5267\u5e7b\u89c9\u3002QueryBandits\u901a\u8fc7\u524d\u5411\u4f20\u9012\u673a\u5236\u5728\u7ebf\u5b66\u4e60\u8bed\u4e49\u7279\u5f81\u7b56\u7565\uff0c\u53ef\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u68af\u5ea6\u8c03\u6574\u3002"}}
{"id": "2602.21114", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21114", "abs": "https://arxiv.org/abs/2602.21114", "authors": ["Tianyu Lu", "Jiajun He", "Mohammadali Mohammadi", "Michail Matthaiou"], "title": "Delay Alignment Modulation for Secure ISAC Systems", "comment": null, "summary": "This paper introduces delay-alignment modulation (DAM) for secure integrated sensing and communication (ISAC). Due to the broadcast nature of multi-user downlinks, communications are vulnerable to eavesdropping. DAM applies controlled per-path symbol delays at the transmitter to coherently align the multipath components at the intended user, enhancing the received signal power, while simultaneously creating delay misalignment at the eavesdropper (Eve). To mitigate sensing degradation caused by multipath propagation, we propose a two-stage protocol that first estimates the angle and then the delay of the line-of-sight (LoS) path after suppressing multipath interference. We derive the secrecy spectral efficiency (SSE) and the Cramer-Rao (CRB) of the target delay. Finally, we develop a path-based zero-forcing (ZF) precoding framework and formulate a max-min SSE design under CRB and power constraints. Simulation results show DAM significantly outperforms the strongest-path (SP) benchmark in terms of SSE, while meeting sensing requirements, since intentional delay alignment at legitimate users degrades reception at Eve.", "AI": {"tldr": "DAM\u6280\u672f\u901a\u8fc7\u63a7\u5236\u591a\u5f84\u5ef6\u8fdf\u5bf9\u9f50\u589e\u5f3a\u5408\u6cd5\u7528\u6237\u4fe1\u53f7\uff0c\u540c\u65f6\u4f7f\u7a83\u542c\u8005\u4fe1\u53f7\u5931\u914d\uff0c\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u591a\u7528\u6237\u4e0b\u884c\u94fe\u8def\u7684\u5e7f\u64ad\u7279\u6027\u4f7f\u901a\u4fe1\u6613\u53d7\u7a83\u542c\uff0c\u540c\u65f6\u591a\u5f84\u4f20\u64ad\u4f1a\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u589e\u5f3a\u901a\u4fe1\u5b89\u5168\u548c\u611f\u77e5\u51c6\u786e\u6027\u7684\u6280\u672f\u3002", "method": "\u63d0\u51fa\u5ef6\u8fdf\u5bf9\u9f50\u8c03\u5236(DAM)\uff0c\u5728\u53d1\u5c04\u7aef\u63a7\u5236\u6bcf\u5f84\u7b26\u53f7\u5ef6\u8fdf\uff0c\u4f7f\u591a\u5f84\u5206\u91cf\u5728\u5408\u6cd5\u7528\u6237\u5904\u76f8\u5e72\u5bf9\u9f50\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u534f\u8bae\u5148\u4f30\u8ba1\u89d2\u5ea6\u518d\u4f30\u8ba1LoS\u8def\u5f84\u5ef6\u8fdf\uff1b\u5f00\u53d1\u57fa\u4e8e\u8def\u5f84\u7684ZF\u9884\u7f16\u7801\u6846\u67b6\uff0c\u5728CRB\u548c\u529f\u7387\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u6700\u5c0fSSE\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aDAM\u5728SSE\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5f3a\u5f84\u57fa\u51c6\u65b9\u6848\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u8981\u6c42\uff0c\u56e0\u4e3a\u6545\u610f\u5ef6\u8fdf\u5bf9\u9f50\u5728\u5408\u6cd5\u7528\u6237\u5904\u589e\u5f3a\u4fe1\u53f7\u800c\u5728\u7a83\u542c\u8005\u5904\u964d\u4f4e\u63a5\u6536\u8d28\u91cf\u3002", "conclusion": "DAM\u4e3a\u5b89\u5168ISAC\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5ef6\u8fdf\u5bf9\u9f50\u589e\u5f3a\u5408\u6cd5\u7528\u6237\u4fe1\u53f7\u540c\u65f6\u4f7f\u7a83\u542c\u8005\u4fe1\u53f7\u5931\u914d\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u5b89\u5168\u548c\u611f\u77e5\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2602.20459", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20459", "abs": "https://arxiv.org/abs/2602.20459", "authors": ["Anirudh Ajith", "Amanpreet Singh", "Jay DeYoung", "Nadav Kunievsky", "Austin C. Kozlowski", "Oyvind Tafjord", "James Evans", "Daniel S. Weld", "Tom Hope", "Doug Downey"], "title": "PreScience: A Benchmark for Forecasting Scientific Contributions", "comment": "10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm", "summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PreScience\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u80fd\u5426\u57fa\u4e8e\u5386\u53f2\u79d1\u5b66\u8bb0\u5f55\u9884\u6d4b\u672a\u6765\u7684\u79d1\u5b66\u53d1\u5c55\uff0c\u5305\u542b\u5408\u4f5c\u8005\u9884\u6d4b\u3001\u53c2\u8003\u6587\u732e\u9009\u62e9\u3001\u8d21\u732e\u751f\u6210\u548c\u5f71\u54cd\u529b\u9884\u6d4b\u56db\u4e2a\u4efb\u52a1\uff0c\u53d1\u73b0\u5f53\u524dAI\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22AI\u7cfb\u7edf\u80fd\u5426\u57fa\u4e8e\u622a\u6b62\u67d0\u4e2a\u65f6\u95f4\u70b9\u7684\u79d1\u5b66\u8bb0\u5f55\u6765\u9884\u6d4b\u540e\u7eed\u7684\u79d1\u5b66\u53d1\u5c55\u3002\u8fd9\u79cd\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc6\u522b\u5408\u4f5c\u8005\u3001\u53d1\u73b0\u6709\u5f71\u54cd\u529b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u9884\u6d4b\u54ea\u4e9b\u95ee\u9898\u548c\u65b9\u6cd5\u5c06\u6210\u4e3a\u672a\u6765\u7684\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u63d0\u51fa\u4e86PreScience\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u751f\u6210\u4efb\u52a1\uff1a\u5408\u4f5c\u8005\u9884\u6d4b\u3001\u53c2\u8003\u6587\u732e\u9009\u62e9\u3001\u8d21\u732e\u751f\u6210\u548c\u5f71\u54cd\u529b\u9884\u6d4b\u3002\u6784\u5efa\u4e86\u5305\u542b98K\u7bc7AI\u76f8\u5173\u8bba\u6587\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6d88\u6b67\u7684\u4f5c\u8005\u8eab\u4efd\u3001\u65f6\u95f4\u5bf9\u9f50\u7684\u5b66\u672f\u5143\u6570\u636e\uff0c\u4ee5\u53ca\u5305\u542b502K\u7bc7\u8bba\u6587\u7684\u4f5c\u8005\u53d1\u8868\u5386\u53f2\u548c\u5f15\u7528\u5173\u7cfb\u7684\u7ed3\u6784\u5316\u56fe\u3002\u5f00\u53d1\u4e86LACERScore\u8fd9\u4e00\u65b0\u7684LLM-based\u8d21\u732e\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6bcf\u4e2a\u4efb\u52a1\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff1a\u5728\u8d21\u732e\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u524d\u6cbfLLM\u4e0e\u771f\u5b9e\u8d21\u732e\u7684\u76f8\u4f3c\u5ea6\u4ec5\u4e3a\u4e2d\u7b49\u6c34\u5e73\uff08GPT-5\u5e73\u5747\u5f97\u52065.6/10\uff09\u3002\u5f53\u5c06\u56db\u4e2a\u4efb\u52a1\u7ec4\u5408\u621012\u4e2a\u6708\u7684\u7aef\u5230\u7aef\u79d1\u5b66\u751f\u4ea7\u6a21\u62df\u65f6\uff0c\u751f\u6210\u7684\u5408\u6210\u8bed\u6599\u5e93\u5728\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u90fd\u7cfb\u7edf\u6027\u5730\u4f4e\u4e8e\u540c\u671f\u4eba\u7c7b\u64b0\u5199\u7684\u7814\u7a76\u3002", "conclusion": "AI\u7cfb\u7edf\u57fa\u4e8e\u5386\u53f2\u79d1\u5b66\u8bb0\u5f55\u9884\u6d4b\u672a\u6765\u79d1\u5b66\u53d1\u5c55\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u591a\u6837\u5316\u548c\u65b0\u9896\u7684\u7814\u7a76\u65b9\u9762\u3002PreScience\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u79d1\u5b66\u9884\u6d4b\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0cLACERScore\u4f5c\u4e3a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002"}}
{"id": "2602.20409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20409", "abs": "https://arxiv.org/abs/2602.20409", "authors": ["Mainak Singha", "Sarthak Mehrotra", "Paolo Casari", "Subhasis Chaudhuri", "Elisa Ricci", "Biplab Banerjee"], "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation", "comment": "Accepted in CVPR 2026", "summary": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.", "AI": {"tldr": "CLIPoint3D\uff1a\u9996\u4e2a\u57fa\u4e8eCLIP\u7684\u5c11\u6837\u672c\u65e0\u76d1\u77633D\u70b9\u4e91\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6df1\u5ea6\u56fe\u6295\u5f71\u3001\u77e5\u8bc6\u9a71\u52a8\u63d0\u793a\u8c03\u4f18\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u5408\u6210\u5230\u771f\u5b9e\u70b9\u4e91\u57df\u9002\u5e94\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u57df\u9002\u5e94\u65b9\u9762\u8106\u5f31\uff0c\u7279\u522b\u662f\u5728\u4ece\u5408\u6210\u70b9\u4e91\u5230\u771f\u5b9e\u70b9\u4e91\u7684\u9002\u5e94\u4e2d\u3002\u4f20\u7edf3D\u57df\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u91cd\u578b\u53ef\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u867d\u7136\u51c6\u786e\u7387\u9ad8\u4f46\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u5c063D\u6837\u672c\u6295\u5f71\u5230\u591a\u4e2a\u6df1\u5ea6\u56fe\uff1b2) \u5229\u7528\u51bb\u7ed3\u7684CLIP\u4e3b\u5e72\uff0c\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u63d0\u793a\u8c03\u4f18\u65b9\u6848\u8fdb\u884c\u7cbe\u70bc\uff0c\u6574\u5408\u9ad8\u7ea7\u8bed\u8a00\u5148\u9a8c\u548c\u8f7b\u91cf\u7ea73D\u7f16\u7801\u5668\u7684\u51e0\u4f55\u7ebf\u7d22\uff1b3) \u5bf9CLIP\u7f16\u7801\u5668\u5e94\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b4) \u8bbe\u8ba1\u71b5\u5f15\u5bfc\u89c6\u56fe\u91c7\u6837\u7b56\u7565\u9009\u62e9\u7f6e\u4fe1\u6295\u5f71\uff1b5) \u4f7f\u7528\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5bf9\u9f50\u635f\u5931\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u539f\u578b\u5bf9\u9f50\u635f\u5931\u534f\u540c\u6865\u63a5\u6e90-\u76ee\u6807\u5206\u5e03\u5dee\u8ddd\u5e76\u4fdd\u6301\u7c7b\u522b\u53ef\u5206\u6027\u3002", "result": "\u5728PointDA-10\u548cGraspNetPC-10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLIPoint3D\u76f8\u6bd4\u57fa\u4e8eCLIP\u548c\u4f20\u7edf\u7f16\u7801\u5668\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e863-16%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "CLIPoint3D\u662f\u9996\u4e2a\u57fa\u4e8eCLIP\u7684\u5c11\u6837\u672c\u65e0\u76d1\u77633D\u70b9\u4e91\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u6574\u5408\u8bed\u8a00\u5148\u9a8c\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u57df\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2602.21146", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21146", "abs": "https://arxiv.org/abs/2602.21146", "authors": ["Wenlong Wang", "Tianyang Zhang", "Tailun Dong", "Lei Zhang"], "title": "TCDA: Robust 2D-DOA Estimation for Defective L-Shaped Arrays", "comment": "5 pages, 2 figures", "summary": "While tensor-based methods excel at Direction-of-Arrival (DOA) estimation, their performance degrades severely with faulty or sparse arrays that violate the required manifold structure. To address this challenge, we propose Tensor Completion for Defective Arrays (TCDA), a robust algorithm that reformulates the physical imperfection problem as a data recovery task within a virtual tensor space. We present a detailed derivation for constructing an incomplete third-order Parallel Factor Analysis (PARAFAC) tensor from the faulty array signals via subarray partitioning, cross-correlation, and dimensional reshaping. Leveraging the tensor's inherent low-rank structure, an Alternating Least Squares (ALS)-based algorithm directly recovers the factor matrices embedding the DOA parameters from the incomplete observations. This approach provides a software-defined 'self-healing' capability, demonstrating exceptional robustness against random element failures without requiring additional processing steps for DOA estimation.", "AI": {"tldr": "\u63d0\u51faTCDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u8865\u5168\u89e3\u51b3\u6545\u969c\u9635\u5217DOA\u4f30\u8ba1\u95ee\u9898\uff0c\u5c06\u7269\u7406\u7f3a\u9677\u8f6c\u5316\u4e3a\u865a\u62df\u5f20\u91cf\u7a7a\u95f4\u7684\u6570\u636e\u6062\u590d\u4efb\u52a1", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f20\u91cf\u7684DOA\u4f30\u8ba1\u7b97\u6cd5\u5728\u9635\u5217\u5b58\u5728\u6545\u969c\u6216\u7a00\u758f\u7ed3\u6784\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u56e0\u4e3a\u6545\u969c\u9635\u5217\u7834\u574f\u4e86\u6240\u9700\u7684\u6d41\u5f62\u7ed3\u6784", "method": "\u901a\u8fc7\u5b50\u9635\u5217\u5212\u5206\u3001\u4e92\u76f8\u5173\u548c\u7ef4\u5ea6\u91cd\u5851\u6784\u5efa\u4e0d\u5b8c\u6574\u7684\u4e09\u9636PARAFAC\u5f20\u91cf\uff0c\u5229\u7528\u5f20\u91cf\u7684\u56fa\u6709\u4f4e\u79e9\u7ed3\u6784\uff0c\u91c7\u7528\u57fa\u4e8e\u4ea4\u66ff\u6700\u5c0f\u4e8c\u4e58\u7684\u7b97\u6cd5\u76f4\u63a5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u6062\u590d\u5305\u542bDOA\u53c2\u6570\u7684\u56e0\u5b50\u77e9\u9635", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8f6f\u4ef6\u5b9a\u4e49\u7684\"\u81ea\u6108\"\u80fd\u529b\uff0c\u5bf9\u968f\u673a\u5143\u7d20\u6545\u969c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u7684DOA\u4f30\u8ba1\u5904\u7406\u6b65\u9aa4", "conclusion": "TCDA\u7b97\u6cd5\u6210\u529f\u5c06\u7269\u7406\u9635\u5217\u7f3a\u9677\u95ee\u9898\u8f6c\u5316\u4e3a\u5f20\u91cf\u8865\u5168\u95ee\u9898\uff0c\u4e3a\u6545\u969c\u9635\u5217\u7684DOA\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20372", "abs": "https://arxiv.org/abs/2602.20372", "authors": ["Chundra Cathcart", "Arne Rubehn", "Katja Bocklage", "Luca Ciucci", "Kellen Parker van Dam", "Al\u017eb\u011bta Ku\u010derov\u00e1", "Jekaterina Ma\u017eara", "Carlo Y. Meloni", "David Snee", "Johann-Mattis List"], "title": "How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity", "comment": null, "summary": "Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u5728\u6c9f\u901a\u6548\u7387\u4e0a\u4e0d\u5982\u9884\u671f\uff0c\u6311\u6218\u4e86\u7cbe\u786e\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u80fd\u4f18\u5316\u6548\u7387\u7684\u89c2\u70b9", "motivation": "\u5148\u524d\u7814\u7a76\u8ba4\u4e3a\u7cbe\u786e\u9012\u5f52\u6570\u5b57\u7cfb\u7edf\u901a\u8fc7\u5e73\u8861\u6570\u5b57\u8bcd\u6c47\u5e93\u5927\u5c0f\u548c\u6570\u5b57\u672f\u8bed\u7684\u5e73\u5747\u5f62\u6001\u53e5\u6cd5\u590d\u6742\u6027\u6765\u4f18\u5316\u6c9f\u901a\u6548\u7387\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u7814\u7a76\u672a\u80fd\u5145\u5206\u89e3\u91ca\u8bed\u8a00\u6240\u8868\u73b0\u51fa\u7684\u590d\u6742\u6027\u7a0b\u5ea6", "method": "\u4f7f\u7528\u6765\u81ea52\u79cd\u9057\u4f20\u591a\u6837\u8bed\u8a00\u7684\u6570\u636e\uff0c\u91c7\u7528\u533a\u5206\u53ef\u9884\u6d4b\u548c\u4e0d\u53ef\u9884\u6d4b\u5f02\u5f62\uff08\u5f62\u5f0f\u53d8\u5f02\uff09\u7684\u6ce8\u91ca\u65b9\u6848\u8fdb\u884c\u5206\u6790", "result": "\u7814\u7a76\u8868\u660e\u4e16\u754c\u4e0a\u8bb8\u591a\u8bed\u8a00\u7684\u6570\u5b57\u7cfb\u7edf\u5728\u6c9f\u901a\u6548\u7387\u4e0a\u660e\u663e\u4f4e\u4e8e\u9884\u671f\u6c34\u5e73", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u6570\u5b57\u7cfb\u7edf\u7814\u7a76\u548c\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u6f14\u5316\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u6311\u6218\u4e86\u73b0\u6709\u5173\u4e8e\u6570\u5b57\u7cfb\u7edf\u6548\u7387\u4f18\u5316\u7684\u7406\u8bba"}}
{"id": "2602.20494", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20494", "abs": "https://arxiv.org/abs/2602.20494", "authors": ["Haotian Si", "Changhua Pei", "Xiao He", "Zeyan Li", "Zhe Xie", "Zexin Wang", "Jiyao Hu", "Zhaoyang Yu", "Tieying Zhang", "Dan Pei", "Jianhui Li", "Gaogang Xie"], "title": "KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning", "comment": null, "summary": "Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u6761\u4ef6\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5f00\u53d1\u51faKairosVL\u6a21\u578b\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65e5\u76ca\u590d\u6742\u548c\u51b3\u7b56\u5bfc\u5411\u7684\u9700\u6c42\uff0c\u4f20\u7edf\u6570\u503c\u5efa\u6a21\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u8981\uff0c\u9700\u8981\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u6269\u5c55\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u4e24\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u8f6e\u5f3a\u5316\u6a21\u578b\u5bf9\u57fa\u672c\u65f6\u95f4\u539f\u8bed\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7b2c\u4e8c\u8f6e\u4e13\u6ce8\u4e8e\u8bed\u4e49\u6761\u4ef6\u63a8\u7406\u3002\u5f00\u53d1\u51faKairosVL\u6a21\u578b\u5b9e\u73b0\u8fd9\u4e00\u6846\u67b6\u3002", "result": "KairosVL\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\uff0c\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u4fdd\u6301\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u5efa\u6a21\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u6ee1\u8db3\u4e86\u5f53\u524d\u5bf9\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2602.20412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20412", "abs": "https://arxiv.org/abs/2602.20412", "authors": ["Aayush Dhakal", "Subash Khanal", "Srikumar Sastry", "Jacob Arndt", "Philipe Ambrozio Dias", "Dalton Lunga", "Nathan Jacobs"], "title": "SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images", "comment": "Accepted to CVPR 2026", "summary": "The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\\% accuracy and +69.62\\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.", "AI": {"tldr": "SimLBR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6df7\u5408\u6b63\u5219\u5316\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56f4\u7ed5\u771f\u5b9e\u56fe\u50cf\u5206\u5e03\u5b66\u4e60\u7d27\u5bc6\u51b3\u7b56\u8fb9\u754c\uff0c\u5c06\u865a\u5047\u7c7b\u522b\u89c6\u4e3a\u6c47\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u8de8\u751f\u6210\u5668\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u865a\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u5177\u6709\u5f3a\u5206\u5e03\u504f\u79fb\u7684\u786c\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u707e\u96be\u6027\u5931\u8d25\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSimLBR\u6846\u67b6\uff0c\u91c7\u7528\u6f5c\u5728\u6df7\u5408\u6b63\u5219\u5316(LBR)\u6280\u672f\uff0c\u5b66\u4e60\u56f4\u7ed5\u771f\u5b9e\u56fe\u50cf\u5206\u5e03\u7684\u7d27\u5bc6\u51b3\u7b56\u8fb9\u754c\uff0c\u5c06\u865a\u5047\u7c7b\u522b\u89c6\u4e3a\u6c47\u7c7b\u800c\u975e\u72ec\u7acb\u7c7b\u522b\u3002", "result": "\u5728Chameleon\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSimLBR\u5b9e\u73b0\u4e86\u9ad8\u8fbe+24.85%\u7684\u51c6\u786e\u7387\u548c+69.62%\u7684\u53ec\u56de\u7387\u63d0\u5347\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "SimLBR\u4e3aAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u751f\u6210\u5668\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u53ef\u9760\u6027\u5bfc\u5411\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.21162", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21162", "abs": "https://arxiv.org/abs/2602.21162", "authors": ["Hao Feng", "Ebrahim Bedeer", "Ming Zeng", "Xingwang Li", "Shimin Gong", "Quoc-Viet Pham"], "title": "Phase-Aware Localization in Pinching Antenna Systems: CRLB Analysis and ML Estimation", "comment": "4 pages, 2 figures; submitted to IEEE journals", "summary": "Pinching antenna systems (PASS) have recently emerged as a promising architecture for high-frequency wireless communications. In this letter, we investigate localization in PASS by jointly exploiting the received signal amplitude and phase information, unlike recent works that consider only the amplitude information. A complex baseband signal model is formulated to capture free-space path loss, waveguide attenuation, and distance-dependent phase rotation between the user and each pinching antenna. Using this model, we derive the Fisher information matrix (FIM) with respect to the user location and obtain closed-form expressions for the Cramer-Rao lower bound (CRLB) and the position error bound (PEB). A maximum likelihood (ML) estimator that jointly considers the received signal amplitude and phase is developed to estimate the unknown user location. Given the non-convexity of the estimation problem, a two-stage solution combining coarse grid search and Levenberg-Marquardt refinement is proposed. Numerical results demonstrate that the proposed phase-aware estimator outperforms existing amplitude-only method in terms of positioning accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e2d\u5229\u7528\u63a5\u6536\u4fe1\u53f7\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u8fdb\u884c\u7528\u6237\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u73b0\u6709\u4ec5\u4f7f\u7528\u5e45\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u662f\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u7684\u6709\u524d\u666f\u67b6\u6784\uff0c\u73b0\u6709\u5b9a\u4f4d\u65b9\u6cd5\u4ec5\u5229\u7528\u5e45\u5ea6\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u76f8\u4f4d\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8054\u5408\u5229\u7528\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u6765\u63d0\u9ad8\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u5efa\u7acb\u4e86\u5305\u542b\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\u3001\u6ce2\u5bfc\u8870\u51cf\u548c\u8ddd\u79bb\u76f8\u5173\u76f8\u4f4d\u65cb\u8f6c\u7684\u590d\u57fa\u5e26\u4fe1\u53f7\u6a21\u578b\uff1b\u63a8\u5bfc\u4e86\u5173\u4e8e\u7528\u6237\u4f4d\u7f6e\u7684Fisher\u4fe1\u606f\u77e9\u9635\u548cCRLB/PEB\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff1b\u5f00\u53d1\u4e86\u8054\u5408\u8003\u8651\u5e45\u5ea6\u548c\u76f8\u4f4d\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff1b\u9488\u5bf9\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7c97\u7f51\u683c\u641c\u7d22\u548cLevenberg-Marquardt\u7ec6\u5316\u7684\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u76f8\u4f4d\u611f\u77e5\u4f30\u8ba1\u5668\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u4ec5\u5e45\u5ea6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u63a5\u6536\u4fe1\u53f7\u7684\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u76f8\u4f4d\u4fe1\u606f\u5bf9\u5b9a\u4f4d\u6027\u80fd\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2602.21167", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.21167", "abs": "https://arxiv.org/abs/2602.21167", "authors": ["Hao Feng", "Ming Zeng", "Ebrahim Bedeer", "Xingwang Li", "Octavia A. Dobre", "Zhiguo Ding"], "title": "Wireless-Fed Pinching-Antenna Systems with Horn Antennas", "comment": "4 pages; 1 figure; submitted to IEEE journals", "summary": "Pinching-antenna systems have recently emerged as a promising solution for enhancing coverage in high-frequency wireless communications by guiding signals through dielectric waveguides and radiating them via position-adjustable antennas. However, their practical deployment is fundamentally constrained by waveguide attenuation and line-installation requirements, which limit the achievable coverage range. To address this challenge, this paper investigates a wireless-fed pinching-antenna architecture that employs highly directional horn antennas to enable efficient coverage extension. Specifically, a full-duplex amplify-and-forward relay equipped with horn antennas is introduced between the base station and the waveguide input, which significantly improves the link budget in high-frequency bands while effectively eliminating self-interference. On this basis, we formulate a total power minimization problem subject to a quality-of-service constraint at the user equipment, involving the joint optimization of the pinching-antenna position, the relay amplification gain, and the base station transmit power. By exploiting the structure of the end-to-end signal-to-noise ratio, the optimal pinching-antenna position is first obtained in closed form by balancing waveguide attenuation and free-space path loss. Subsequently, closed-form expressions for the optimal relay gain and transmit power are derived. Numerical results demonstrate that the proposed scheme substantially outperforms conventional systems without relaying and relay-assisted transmission with fixed antennas in terms of total power consumption.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\uff0c\u4f7f\u7528\u9ad8\u5b9a\u5411\u5587\u53ed\u5929\u7ebf\u548c\u5168\u53cc\u5de5\u4e2d\u7ee7\u6765\u6269\u5c55\u9ad8\u9891\u901a\u4fe1\u8986\u76d6\u8303\u56f4\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u3001\u4e2d\u7ee7\u589e\u76ca\u548c\u57fa\u7ad9\u529f\u7387\u6765\u6700\u5c0f\u5316\u603b\u529f\u8017\u3002", "motivation": "\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u901a\u8fc7\u4ecb\u8d28\u6ce2\u5bfc\u548c\u53ef\u8c03\u5929\u7ebf\u589e\u5f3a\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u8986\u76d6\uff0c\u4f46\u53d7\u9650\u4e8e\u6ce2\u5bfc\u8870\u51cf\u548c\u7ebf\u8def\u5b89\u88c5\u8981\u6c42\uff0c\u8986\u76d6\u8303\u56f4\u6709\u9650\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8986\u76d6\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\uff0c\u5728\u57fa\u7ad9\u548c\u6ce2\u5bfc\u8f93\u5165\u4e4b\u95f4\u5f15\u5165\u914d\u5907\u5587\u53ed\u5929\u7ebf\u7684\u5168\u53cc\u5de5\u653e\u5927\u8f6c\u53d1\u4e2d\u7ee7\uff0c\u6d88\u9664\u81ea\u5e72\u6270\u3002\u5efa\u7acb\u603b\u529f\u7387\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u5939\u6301\u5929\u7ebf\u4f4d\u7f6e\u3001\u4e2d\u7ee7\u589e\u76ca\u548c\u57fa\u7ad9\u53d1\u5c04\u529f\u7387\uff0c\u901a\u8fc7\u5206\u6790\u7aef\u5230\u7aef\u4fe1\u566a\u6bd4\u7ed3\u6784\u83b7\u5f97\u95ed\u5f0f\u89e3\u3002", "result": "\u63a8\u5bfc\u51fa\u6700\u4f18\u5939\u6301\u5929\u7ebf\u4f4d\u7f6e\u7684\u95ed\u5f0f\u89e3\uff08\u5e73\u8861\u6ce2\u5bfc\u8870\u51cf\u548c\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\uff09\uff0c\u4ee5\u53ca\u6700\u4f18\u4e2d\u7ee7\u589e\u76ca\u548c\u53d1\u5c04\u529f\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u603b\u529f\u8017\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u65e0\u4e2d\u7ee7\u7684\u4f20\u7edf\u7cfb\u7edf\u548c\u56fa\u5b9a\u5929\u7ebf\u4e2d\u7ee7\u8f85\u52a9\u4f20\u8f93\u3002", "conclusion": "\u65e0\u7ebf\u9988\u7535\u5939\u6301\u5929\u7ebf\u67b6\u6784\u901a\u8fc7\u9ad8\u5b9a\u5411\u5587\u53ed\u5929\u7ebf\u548c\u5168\u53cc\u5de5\u4e2d\u7ee7\u6709\u6548\u6269\u5c55\u9ad8\u9891\u901a\u4fe1\u8986\u76d6\u8303\u56f4\uff0c\u8054\u5408\u4f18\u5316\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u603b\u529f\u8017\uff0c\u4e3a\u9ad8\u9891\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20517", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20517", "abs": "https://arxiv.org/abs/2602.20517", "authors": ["Rakshit Trivedi", "Kartik Sharma", "David C Parkes"], "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination", "comment": "Spotlight paper at NeurIPS 2025", "summary": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.", "AI": {"tldr": "MIMIC\u6846\u67b6\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u7684\u5185\u90e8\u8868\u793a\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u5185\u90e8\u8bed\u97f3\uff0c\u518d\u57fa\u4e8e\u6269\u6563\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\uff0c\u5b9e\u73b0\u4e86\u5728\u63a8\u7406\u65f6\u5bf9\u884c\u4e3a\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u7279\u6027\uff0c\u4e14\u7f3a\u4e4f\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u884c\u4e3a\u7684\u80fd\u529b\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u5185\u90e8\u8bed\u97f3\u6307\u5bfc\u52a8\u4f5c\u9009\u62e9\u7684\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u5185\u90e8\u8868\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMIMIC\u6846\u67b6\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bed\u8a00\u652f\u67b6\u8bad\u7ec3\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ece\u89c2\u5bdf\u4e2d\u751f\u6210\u5185\u90e8\u8bed\u97f3\uff1b2) \u57fa\u4e8e\u6269\u6563\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\uff0c\u6839\u636e\u5f53\u524d\u89c2\u5bdf\u548c\u751f\u6210\u7684\u5185\u90e8\u8bed\u97f3\u9009\u62e9\u52a8\u4f5c\uff1b3) \u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7279\u5b9a\u884c\u4e3a\u8bed\u97f3\u6761\u4ef6\u5316\u667a\u80fd\u4f53\u5b9e\u73b0\u884c\u4e3a\u5f15\u5bfc\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u548c\u4eba\u673a\u534f\u4f5c\u6e38\u620f\u4e2d\uff0cMIMIC\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4e3a\u591a\u6837\u6027\u548c\u5bf9\u4eba\u7c7b\u6f14\u793a\u7684\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u6f14\u793a\u8bad\u7ec3\u7684\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5f15\u5bfc\u3002", "conclusion": "MIMIC\u901a\u8fc7\u5c06\u8bed\u8a00\u4f5c\u4e3a\u884c\u4e3a\u610f\u56fe\u7684\u5185\u90e8\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f53\u524d\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u6355\u6349\u884c\u4e3a\u591a\u6837\u6027\u548c\u63a8\u7406\u65f6\u884c\u4e3a\u5f15\u5bfc\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4eba\u673a\u534f\u8c03\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20423", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20423", "abs": "https://arxiv.org/abs/2602.20423", "authors": ["Taha Koleilat", "Hojat Asgariandehkordi", "Omid Nejati Manzari", "Berardino Barile", "Yiming Xiao", "Hassan Rivaz"], "title": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation", "comment": "CVPR 2026; Project Page: https://tahakoleilat.github.io/MedCLIPSeg", "summary": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.", "AI": {"tldr": "MedCLIPSeg\uff1a\u57fa\u4e8eCLIP\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u9886\u57df\u6cdb\u5316\u80fd\u529b", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u89e3\u5256\u7279\u5f81\u6a21\u7cca\u548c\u9886\u57df\u504f\u79fb\u7b49\u6311\u6218\u3002\u867d\u7136CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5bc6\u96c6\u3001\u6587\u672c\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u63d0\u51faMedCLIPSeg\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5229\u7528patch\u7ea7CLIP\u5d4c\u5165\uff0c\u5b9e\u73b0\u56fe\u50cf\u548c\u6587\u672ctoken\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u7ed3\u5408\u8f6fpatch\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u4fc3\u8fdb\u4e0d\u540c\u6587\u672c\u63d0\u793a\u95f4\u7684\u8bed\u4e49\u5b66\u4e60", "result": "\u5728\u6db5\u76d65\u79cd\u6210\u50cf\u6a21\u6001\u548c6\u4e2a\u5668\u5b98\u768416\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedCLIPSeg\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\u6765\u7a81\u51fa\u5206\u5272\u7ed3\u679c\u7684\u5c40\u90e8\u53ef\u9760\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u6982\u7387\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u5728\u6587\u672c\u9a71\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u3001\u9886\u57df\u6cdb\u5316\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.20476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20476", "abs": "https://arxiv.org/abs/2602.20476", "authors": ["Anindita Ghosh", "Vladislav Golyanik", "Taku Komura", "Philipp Slusallek", "Christian Theobalt", "Rishabh Dabral"], "title": "SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens", "comment": "13 pages, 6 figures, 4 tables", "summary": "Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent (\"walk to the couch\") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.", "AI": {"tldr": "SceMoS\uff1a\u57fa\u4e8e2D\u573a\u666f\u8868\u793a\u76843D\u4eba\u4f53\u8fd0\u52a8\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u6267\u884c\uff0c\u4f7f\u7528BEV\u56fe\u50cf\u548c\u5c40\u90e8\u9ad8\u5ea6\u56fe\u66ff\u4ee3\u6602\u8d35\u76843D\u76d1\u7763\uff0c\u5728\u4fdd\u6301\u7269\u7406\u771f\u5b9e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e3D\u573a\u666f\u6570\u636e\uff08\u70b9\u4e91\u3001\u4f53\u7d20\u7f51\u683c\uff09\u7684\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u540c\u65f6\u5b66\u4e60\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a5\u89e6\u63a8\u7406\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5408\u6210\u8bed\u4e49\u610f\u56fe\u660e\u786e\u4e14\u7269\u7406\u53ef\u884c\u7684\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u3002", "method": "1. \u6587\u672c\u6761\u4ef6\u81ea\u56de\u5f52\u5168\u5c40\u8fd0\u52a8\u89c4\u5212\u5668\uff1a\u4f7f\u7528DINOv2\u7279\u5f81\u7f16\u7801\u7684\u9e1f\u77b0\u56fe\u4f5c\u4e3a\u573a\u666f\u8868\u793a\uff1b2. \u51e0\u4f55\u57fa\u7840\u8fd0\u52a8\u5206\u8bcd\u5668\uff1a\u901a\u8fc7\u6761\u4ef6VQ-VAE\u8bad\u7ec3\uff0c\u4f7f\u75282D\u5c40\u90e8\u573a\u666f\u9ad8\u5ea6\u56fe\uff0c\u5c06\u8868\u9762\u7269\u7406\u76f4\u63a5\u5d4c\u5165\u79bb\u6563\u8bcd\u6c47\u8868\u3002", "result": "\u5728TRUMANS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u63a5\u89e6\u7cbe\u5ea6\uff0c\u573a\u666f\u7f16\u7801\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u8d85\u8fc750%\uff0c\u8bc1\u660e2D\u573a\u666f\u7ebf\u7d22\u80fd\u6709\u6548\u652f\u64913D\u4eba-\u573a\u666f\u4ea4\u4e92\u3002", "conclusion": "\u7ed3\u6784\u53162D\u573a\u666f\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u5b8c\u65743D\u76d1\u7763\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u6548\u7387\u4e0e\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\uff1aBEV\u8bed\u4e49\u6355\u83b7\u7a7a\u95f4\u5e03\u5c40\u548c\u53ef\u4f9b\u6027\u7528\u4e8e\u5168\u5c40\u63a8\u7406\uff0c\u5c40\u90e8\u9ad8\u5ea6\u56fe\u786e\u4fdd\u7ec6\u7c92\u5ea6\u7269\u7406\u9075\u5faa\u800c\u65e0\u9700\u5b8c\u65743D\u4f53\u79ef\u63a8\u7406\u3002"}}
{"id": "2602.20528", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20528", "abs": "https://arxiv.org/abs/2602.20528", "authors": ["Justin Lovelace", "Christian Belardi", "Sofian Zalouk", "Adhitya Polavaram", "Srivatsa Kundurthy", "Kilian Q. Weinberger"], "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning", "comment": "COLM 2025", "summary": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.", "AI": {"tldr": "STAR-LDM\u662f\u4e00\u79cd\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u89c4\u5212\u548c\u81ea\u56de\u5f52\u751f\u6210\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\"\u601d\u8003\"\u9636\u6bb5\u5728\u8fde\u7eed\u7a7a\u95f4\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u518d\u751f\u6210\u79bb\u6563\u6807\u8bb0\uff0c\u5728\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u9010\u6807\u8bb0\u51b3\u7b56\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c4\u5212\u80fd\u529b\u3002STAR-LDM\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u6269\u6563\u89c4\u5212\u673a\u5236\uff0c\u8ba9\u6a21\u578b\u5728\u751f\u6210\u524d\u8fdb\u884c\"\u601d\u8003\"\uff0c\u5728\u8fde\u7eed\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "STAR-LDM\u6574\u5408\u4e86\u6f5c\u5728\u6269\u6563\u89c4\u5212\u548c\u81ea\u56de\u5f52\u751f\u6210\u3002\u6a21\u578b\u5305\u542b\u4e00\u4e2a\"\u601d\u8003\"\u9636\u6bb5\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6682\u505c\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5728\u8fde\u7eed\u7a7a\u95f4\u7cbe\u70bc\u8bed\u4e49\u8ba1\u5212\uff0c\u7136\u540e\u518d\u7ee7\u7eed\u81ea\u56de\u5f52\u751f\u6210\u79bb\u6563\u6807\u8bb0\u3002\u67b6\u6784\u8fd8\u652f\u6301\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fdb\u884c\u76f4\u63a5\u63a7\u5236\u3002", "result": "STAR-LDM\u5728\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u4f18\u4e8e\u76f8\u4f3c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5728LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u6bd4\u8f83\u4e2d\uff0c\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u83b7\u5f97\u8d85\u8fc770%\u7684\u80dc\u7387\u3002\u67b6\u6784\u5141\u8bb8\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u6d41\u7545\u6027\u548c\u63a7\u5236\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "STAR-LDM\u901a\u8fc7\u6574\u5408\u6269\u6563\u89c4\u5212\u548c\u81ea\u56de\u5f52\u751f\u6210\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5168\u5c40\u89c4\u5212\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u652f\u6301\u6709\u6548\u7684\u63a7\u5236\u673a\u5236\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.20550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20550", "abs": "https://arxiv.org/abs/2602.20550", "authors": ["Chengshuai Yang"], "title": "The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation", "comment": null, "summary": "Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u6240\u6709\u6210\u50cf\u524d\u5411\u6a21\u578b\u90fd\u53ef\u752811\u4e2a\u57fa\u672c\u539f\u8bed\u6784\u5efa\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fd1\u4f3c\u8868\u793a\uff0c\u5efa\u7acb\u4e86\u7269\u7406\u4e16\u754c\u6a21\u578b\u7684\u6570\u5b66\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u6210\u50cf\u524d\u5411\u6a21\u578b\u901a\u5e38\u4f5c\u4e3a\u7279\u5b9a\u6a21\u6001\u7684\u5355\u4e00\u4ee3\u7801\u5b9e\u73b0\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5e7f\u6cdb\u7684\u6210\u50cf\u6a21\u6001\u5efa\u7acb\u7edf\u4e00\u7684\u8868\u793a\u7406\u8bba\u3002", "method": "\u5b9a\u4e49\u4e86\u6210\u50cf\u64cd\u4f5c\u7b26\u7c7bCimg\uff0c\u8bc1\u660e\u5176\u4e2d\u6240\u6709\u524d\u5411\u6a21\u578b\u90fd\u53ef\u752811\u4e2a\u57fa\u672c\u539f\u8bed\u6784\u5efa\u7684\u6709\u5411\u65e0\u73af\u56fe\u03b5\u8fd1\u4f3c\u8868\u793a\u3002\u63d0\u4f9b\u4e86\u6784\u9020\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u539f\u8bed\u5e93\u662f\u6700\u5c0f\u7684\u3002", "result": "\u572831\u4e2a\u7ebf\u6027\u6a21\u6001\u4e0a\u9a8c\u8bc1\u4e86\u03b5<0.01\uff0c\u6700\u591a5\u4e2a\u8282\u70b9\u548c\u6df1\u5ea65\u3002\u4e3a9\u4e2a\u975e\u7ebf\u6027\u6a21\u6001\u63d0\u4f9b\u4e86\u6784\u9020\u6027DAG\u5206\u89e3\u3002\u975e\u7ebf\u6027\u88ab\u5206\u4e3a\u70b9\u5f0f\u6807\u91cf\u51fd\u6570\u548c\u81ea\u6d3d\u8fed\u4ee3\u4e24\u7c7b\u3002", "conclusion": "\u5efa\u7acb\u4e86\u6709\u9650\u539f\u8bed\u57fa\u5b9a\u7406\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u6a21\u578b\u6846\u67b6\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u6210\u50cf\u524d\u5411\u6a21\u578b\u7684\u7edf\u4e00\u8868\u793a\u3002"}}
{"id": "2602.21142", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21142", "abs": "https://arxiv.org/abs/2602.21142", "authors": ["Zhifan Jiang", "Dong Yang", "Vishwesh Nath", "Abhijeet Parida", "Nishad P. Kulkarni", "Ziyue Xu", "Daguang Xu", "Syed Muhammad Anwar", "Holger R. Roth", "Marius George Linguraru"], "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis", "comment": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLUMEN\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u56fe\u50cf\u591a\u4efb\u52a1\u6307\u4ee4\u5fae\u8c03\u4f18\u5316\u7eb5\u5411\u80f8\u90e8X\u5149\u7247\u89e3\u8bfb\uff0c\u63d0\u5347\u9884\u540e\u548c\u8bca\u65ad\u6027\u80fd", "motivation": "\u653e\u5c04\u79d1\u533b\u751f\u5206\u6790\u7eb5\u5411\u5f71\u50cf\u53d8\u5316\u5bf9\u51c6\u786e\u8bca\u65ad\u548c\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u7eb5\u5411\u5206\u6790\u8017\u65f6\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u9884\u540e\u80fd\u529b\u7684\u8bad\u7ec3\u6846\u67b6", "method": "\u63d0\u51faLUMEN\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u591a\u56fe\u50cf\u548c\u591a\u4efb\u52a1\u6307\u4ee4\u5fae\u8c03\u4f18\u5316\u7eb5\u5411CXR\u89e3\u8bfb\uff1b\u6784\u5efa\u5305\u542b\u7eb5\u5411\u7814\u7a76\u7684\u6307\u4ee4\u8ddf\u968f\u6570\u636e\u96c6\uff0c\u652f\u6301\u9884\u540eVQA\u4efb\u52a1\u5f00\u53d1", "result": "\u5728MIMIC-CXR\u548cMedical-Diff-VQA\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728\u8bca\u65adVQA\u4efb\u52a1\u4e0a\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5728\u9884\u540e\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u6f5c\u529b", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u5177\u4e34\u5e8a\u610f\u4e49\u7684\u7eb5\u5411\u653e\u5c04\u5f71\u50cf\u89e3\u8bfb\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4ef7\u503c"}}
{"id": "2602.21175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21175", "abs": "https://arxiv.org/abs/2602.21175", "authors": ["Jianglin Lu", "Simon Jenni", "Kushal Kafle", "Jing Shi", "Handong Zhao", "Yun Fu"], "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models", "comment": null, "summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf\u53ef\u63a7\u7684\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5c06\u7b80\u77ed\u67e5\u8be2\u6269\u5c55\u4e3a\u5305\u542b\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u548c\u8d28\u91cf\u63a7\u5236\u7684\u63cf\u8ff0\u6027\u67e5\u8be2\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5373\u53ef\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u9762\u4e34\u7b80\u77ed\u7528\u6237\u67e5\u8be2\u7684\u6311\u6218\uff1a\u8fd9\u4e9b\u67e5\u8be2\u901a\u5e38\u53ea\u6709\u4e00\u4e24\u4e2a\u8bcd\uff0c\u8bed\u4e49\u6a21\u7cca\uff0c\u5bb9\u6613\u4ea7\u751f\u591a\u79cd\u89c6\u89c9\u89e3\u91ca\u51b2\u7a81\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u56fe\u50cf\u8d28\u91cf\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u67e5\u8be2\u8865\u5168\u51fd\u6570\uff0c\u5c06\u7b80\u77ed\u67e5\u8be2\u6269\u5c55\u4e3a\u5305\u542b\u59ff\u6001\u3001\u573a\u666f\u3001\u7f8e\u5b66\u7b49\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u7684\u63cf\u8ff0\u6027\u67e5\u8be2\u3002\u901a\u8fc7\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u7f8e\u5b66\u8bc4\u5206\u6a21\u578b\u7684\u79bb\u6563\u5316\u8d28\u91cf\u7ea7\u522b\u6765\u6761\u4ef6\u5316\u67e5\u8be2\u8865\u5168\uff0c\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u4e14\u8d28\u91cf\u611f\u77e5\u7684\u67e5\u8be2\u6269\u5c55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u68c0\u7d22\u7ed3\u679c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5f25\u5408\u4e86\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u4e0e\u7b80\u77ed\u7528\u6237\u67e5\u8be2\u4e0d\u660e\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u7cfb\u7edf\u5177\u6709\u7075\u6d3b\u6027\uff08\u517c\u5bb9\u4efb\u4f55\u9884\u8bad\u7ec3VLM\uff09\u3001\u900f\u660e\u6027\uff08\u6269\u5c55\u67e5\u8be2\u53ef\u89e3\u91ca\uff09\u548c\u53ef\u63a7\u6027\uff08\u53ef\u5f15\u5bfc\u81f3\u7528\u6237\u504f\u597d\u8d28\u91cf\u7ea7\u522b\uff09\u4e09\u5927\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u65b0\u8303\u5f0f\u6210\u529f\u89e3\u51b3\u4e86\u7b80\u77ed\u67e5\u8be2\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u67e5\u8be2\u6269\u5c55\u548c\u8d28\u91cf\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21186", "abs": "https://arxiv.org/abs/2602.21186", "authors": ["Haoyi Jiang", "Liu Liu", "Xinjie Wang", "Yonghao He", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang"], "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning", "comment": null, "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.", "AI": {"tldr": "Spa3R\u6846\u67b6\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4ece\u65e0\u59ff\u6001\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u5b66\u4e60\u7edf\u4e00\u3001\u89c6\u89d2\u4e0d\u53d8\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f3D\u6a21\u6001\u6216\u7a7a\u95f4\u6307\u4ee4\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u4ece2D\u89c6\u89c9\u4e2d\u6d8c\u73b0\u7a7a\u95f4\u667a\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u57283D\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u8868\u73b0\u80a4\u6d45\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u663e\u5f0f3D\u6a21\u6001\uff0c\u8981\u4e48\u901a\u8fc7\u90e8\u5206\u89c6\u89d2\u6761\u4ef6\u51e0\u4f55\u5148\u9a8c\u589e\u5f3aVLMs\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u8ba9\u8bed\u8a00\u6a21\u578b\u627f\u62c5\u4e86\u4ece\u7a00\u758f\u7ebf\u7d22\u9690\u5f0f\u91cd\u5efa\u6574\u4f533D\u51e0\u4f55\u7684\u4e0d\u9002\u5b9a\u4efb\u52a1\u3002", "method": "\u63d0\u51faSpa3R\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u6d4b\u6027\u7a7a\u95f4\u573a\u5efa\u6a21(PSFM)\u8303\u5f0f\uff0c\u4ece\u65e0\u59ff\u6001\u591a\u89c6\u89d2\u56fe\u50cf\u5b66\u4e60\u7edf\u4e00\u3001\u89c6\u89d2\u4e0d\u53d8\u7684\u7a7a\u95f4\u8868\u793a\u3002Spa3R\u5b66\u4e60\u4e3a\u4efb\u610f\u672a\u89c1\u89c6\u89d2\u5408\u6210\u7279\u5f81\u573a\uff0c\u6761\u4ef6\u4e8e\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u4ece\u800c\u5185\u5316\u5bf9\u5e95\u5c423D\u573a\u666f\u7684\u6574\u4f53\u8fde\u8d2f\u7406\u89e3\u3002\u901a\u8fc7\u8f7b\u91cf\u9002\u914d\u5668\u5c06\u9884\u8bad\u7ec3\u7684Spa3R\u7f16\u7801\u5668\u96c6\u6210\u5230\u73b0\u6709VLMs\u4e2d\uff0c\u5f62\u6210Spa3-VLM\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684VSI-Bench\u4e0a\uff0cSpa3-VLM\u57283D\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u523058.6%\u7684\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "PSFM\u4e3a\u63a8\u8fdb\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u8bc1\u660e\u7a7a\u95f4\u667a\u80fd\u53ef\u4ee5\u4ece2D\u89c6\u89c9\u4e2d\u81ea\u7136\u6d8c\u73b0\uff0c\u800c\u65e0\u9700\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u6307\u4ee4\u5fae\u8c03\u5f3a\u52a0\u3002"}}
